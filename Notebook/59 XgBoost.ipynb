{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "870d0904-2783-4427-a6f8-118b5d96e140",
   "metadata": {},
   "source": [
    "# üìå Extreme Gradient Boosting (XGBoost)\n",
    "\n",
    "### ‚úÖ What is XGBoost?\n",
    "\n",
    "- **XGBoost is not an algorithm**, it's a **library** built on top of the Gradient Boosting framework.\n",
    "- It combines:\n",
    "  - Machine Learning: **Gradient Boosting**\n",
    "  - Engineering: **Performance Optimization, Parallelism, Hardware Utilization**\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Why XGBoost Uses Gradient Boosting as Base\n",
    "\n",
    "### 1. Flexibility\n",
    "- Can use **any differentiable loss function**.\n",
    "- Works for:\n",
    "  - Regression\n",
    "  - Classification (binary/multiclass)\n",
    "  - Time series\n",
    "  - Ranking\n",
    "  - Anomaly detection\n",
    "\n",
    "### 2. Performance\n",
    "- Fast due to engineering innovations.\n",
    "\n",
    "### 3. Robustness\n",
    "- Can handle **missing values internally**.\n",
    "\n",
    "---\n",
    "\n",
    "## üöÄ Features of XGBoost\n",
    "\n",
    "### 1. ‚öôÔ∏è Flexibility\n",
    "- **Cross-Platform:** Windows, Linux, macOS\n",
    "- **Languages:** Python, R, Julia, Java, Scala, etc.\n",
    "- **Integrations:** scikit-learn, Dask, Spark, MLFlow, Kubernetes, SHAP, LIME, Airflow\n",
    "\n",
    "---\n",
    "\n",
    "### 2. ‚ö° Speed Optimizations\n",
    "\n",
    "#### (a) Parallel Processing (During Tree Building)\n",
    "- Trees are built sequentially.\n",
    "- But **feature splitting is done in parallel** across multiple cores.\n",
    "\n",
    "#### (b) Optimized Data Structure\n",
    "- Uses **Column Block format** (stores data column-wise).\n",
    "- Improves **cache efficiency**.\n",
    "\n",
    "#### (c) Cache Awareness\n",
    "- **Histogram-based training** stores frequently accessed bin values in cache.\n",
    "- Improves speed by reducing memory access time.\n",
    "\n",
    "#### (d) Out-of-Core Computing\n",
    "- Useful for very large datasets.\n",
    "- Loads and trains on **data in chunks**.\n",
    "- Memory-efficient, but works **on a single machine** sequentially.\n",
    "\n",
    "#### (e) Distributed Computing\n",
    "- Data and tasks are split across **multiple machines**.\n",
    "- All machines compute their part ‚Üí a **master node aggregates results**.\n",
    "- Faster training and handling of massive datasets.\n",
    "\n",
    "#### (f) GPU Support\n",
    "- Tree method: `gpu_hist`\n",
    "- Leverages many weak but parallel GPU cores for faster computation.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. üîß Performance Features\n",
    "\n",
    "- **Regularized Learning Objective:** Prevents overfitting (L1 & L2 regularization)\n",
    "- **Handles Missing Values:** Automatically splits on available data\n",
    "- **Sparsity-aware Splits:** Deals with sparse input data\n",
    "- **Approximate Split Finding (Sketching):** For large datasets\n",
    "- **Tree Pruning:** Removes branches that do not improve performance\n",
    "\n",
    "---\n",
    "\n",
    "## üß† Summary\n",
    "\n",
    "| Feature                | Description                                                                 |\n",
    "|------------------------|-----------------------------------------------------------------------------|\n",
    "| Base                  | Gradient Boosting Decision Tree (GBDT)                                     |\n",
    "| Loss Functions        | Any differentiable loss (custom or standard)                               |\n",
    "| Speed                 | Fast due to parallel feature splitting, histogram binning, GPU support      |\n",
    "| Scalability           | Out-of-core (1 machine) & Distributed (multi-machine) supported             |\n",
    "| Integration           | With many platforms, cloud tools, and languages                            |\n",
    "| Handling Missing Data | Automatically handled without preprocessing                                |\n",
    "| Interpretability      | Tools like SHAP, LIME supported                                             |\n",
    "\n",
    "---\n",
    "\n",
    "### üìé Common Tree Methods in XGBoost\n",
    "\n",
    "| Tree Method     | Description                          |\n",
    "|------------------|--------------------------------------|\n",
    "| `exact`         | Exact greedy algorithm                |\n",
    "| `approx`        | Approximate algorithm (sketching)     |\n",
    "| `hist`          | Fast histogram optimized algorithm    |\n",
    "| `gpu_hist`      | GPU accelerated histogram algorithm   |\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15392a89-13e1-4273-8119-1fbab06de762",
   "metadata": {},
   "source": [
    "# üìà XGBoost for Regression\n",
    "\n",
    "## üß† Overview\n",
    "\n",
    "XGBoost for regression follows the **same boosting principle** as Gradient Boosting:\n",
    "\n",
    "1. Start with a **base model** (mean of the target).\n",
    "2. Train a sequence of **Decision Trees** to learn the **residuals** (errors).\n",
    "3. Update the model by adding the new tree's predictions.\n",
    "\n",
    "However, the **tree-building logic in XGBoost** differs significantly:\n",
    "\n",
    "- It does **not use Gini or Entropy** (like classification trees).\n",
    "- It uses a **custom objective function** involving **residuals** and **similarity scores**.\n",
    "\n",
    "---\n",
    "\n",
    "## ü™Ñ Tree Formation in XGBoost\n",
    "\n",
    "### Step 1: Leaf Node Formation\n",
    "\n",
    "Before building the tree, for each **leaf node**, we calculate a **Similarity Score (SS)**:\n",
    "\n",
    "$$\n",
    "SS = \\frac{(\\sum \\text{residuals})^2}{n + \\lambda}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- \\( n \\): number of residuals in the leaf\n",
    "- \\( \\lambda \\): regularization parameter (prevents overfitting)\n",
    "\n",
    "---\n",
    "\n",
    "### Step 2: Gain Calculation\n",
    "\n",
    "When trying to split a node, we calculate the **gain** (improvement in similarity):\n",
    "\n",
    "$$\n",
    "\\text{Gain} = SS_{\\text{Left}} + SS_{\\text{Right}} - SS_{\\text{Parent}} - \\gamma\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- \\( \\gamma \\): regularization term to penalize excessive tree depth (also helps pruning)\n",
    "\n",
    "A split is accepted **only if the gain is positive**.\n",
    "\n",
    "---\n",
    "\n",
    "### Step 3: Tree Building Logic\n",
    "\n",
    "- XGBoost splits based on **maximizing Gain**, not reducing entropy or Gini.\n",
    "- Trees are grown greedily using:\n",
    "  - **Exact Greedy Algorithm** ‚Üí for **small datasets**\n",
    "  - **Approximate Algorithm** ‚Üí for **large datasets**\n",
    "\n",
    "---\n",
    "\n",
    "## üåø Output at Each Leaf Node\n",
    "\n",
    "Once the best splits are found and leaf nodes are created, the output value for each leaf is computed as:\n",
    "\n",
    "$$\n",
    "\\text{Output}_{\\text{leaf}} = \\frac{\\sum \\text{residuals}}{n + \\lambda}\n",
    "$$\n",
    "\n",
    "This output becomes the **prediction from that leaf node**.\n",
    "\n",
    "---\n",
    "\n",
    "## üîÅ Updating the Model (Stage 2)\n",
    "\n",
    "After the first tree (model), we update the prediction:\n",
    "\n",
    "$$\n",
    "F_1(x) = \\text{mean} + \\eta \\cdot f_1(x)\n",
    "$$\n",
    "\n",
    "- \\( \\eta \\): learning rate (default in XGBoost is 0.3)\n",
    "- \\( f_1(x) \\): prediction from first decision tree\n",
    "\n",
    "Then, compute **new residuals**:\n",
    "\n",
    "$$\n",
    "\\text{Residual}_2 = y - F_1(x)\n",
    "$$\n",
    "\n",
    "Train the next decision tree on these residuals.\n",
    "\n",
    "---\n",
    "\n",
    "## üîÑ Summary of Tree Formation Steps\n",
    "\n",
    "1. Start with **mean prediction**.\n",
    "2. Compute **residuals**.\n",
    "3. Use residuals to compute **similarity score** for potential splits.\n",
    "4. Use **gain** to decide best split.\n",
    "5. Continue growing tree using **greedy or approximate algorithms**.\n",
    "6. Update model using:\n",
    "   $$\n",
    "   F_{m}(x) = F_{m-1}(x) + \\eta \\cdot f_m(x)\n",
    "   $$\n",
    "\n",
    "---\n",
    "\n",
    "## üß™ Tree Building Algorithms\n",
    "\n",
    "| Method         | Description                     | Use Case          |\n",
    "|----------------|----------------------------------|-------------------|\n",
    "| Exact Greedy   | Tries all splits                | Small datasets    |\n",
    "| Approximate    | Uses histogram/binning technique | Large datasets    |\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67655766-81d1-4bef-b97d-1f307a331e6d",
   "metadata": {},
   "source": [
    "# üöÄ XGBoost for Classification\n",
    "\n",
    "## üîç Core Idea\n",
    "\n",
    "In classification using XGBoost, we start with a **base model** that estimates the **log-odds**, and then build decision trees sequentially to minimize the error.\n",
    "\n",
    "### Base Model (Model 1):\n",
    "$$\n",
    "\\text{Log(odds)} = \\log_e\\left(\\frac{P}{1 - P}\\right)\n",
    "$$\n",
    "\n",
    "- \\( P \\): Probability of the positive class.\n",
    "- This log-odds is the starting prediction (similar to how mean is used in regression).\n",
    "\n",
    "After calculating log-odds, we convert them into **probabilities**:\n",
    "\n",
    "$$\n",
    "P = \\frac{e^{\\text{log(odds)}}}{1 + e^{\\text{log(odds)}}}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## üå≥ Decision Tree Formation (Model 2 onward)\n",
    "\n",
    "All subsequent models are decision trees trained on **residuals**, similar to gradient boosting.\n",
    "\n",
    "### üìå Similarity Score for Classification\n",
    "\n",
    "XGBoost uses a different formula for classification vs regression. For a given leaf node, the **similarity score (SS)** is:\n",
    "\n",
    "$$\n",
    "SS = \\frac{\\left(\\sum \\text{residuals}\\right)^2}{\\sum \\left(p_i (1 - p_i)\\right) + \\lambda}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- \\( p_i \\): predicted probability for the \\( i \\)-th data point\n",
    "- \\( \\lambda \\): regularization parameter\n",
    "- Residual = actual - predicted (for classification)\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Goal of Tree Building\n",
    "\n",
    "- Try different splits in input columns.\n",
    "- For each potential split, compute **gain** based on change in similarity score:\n",
    "\n",
    "$$\n",
    "\\text{Gain} = SS_{\\text{Left}} + SS_{\\text{Right}} - SS_{\\text{Parent}} - \\gamma\n",
    "$$\n",
    "\n",
    "- Choose the split that maximizes this gain.\n",
    "- Repeat to grow tree until stopping criteria is met.\n",
    "\n",
    "---\n",
    "\n",
    "## üçÉ Leaf Output Calculation\n",
    "\n",
    "Once a node is created, calculate its output (log-odds shift):\n",
    "\n",
    "$$\n",
    "\\text{Output}_{\\text{leaf}} = \\frac{\\sum \\text{residuals}}{\\sum p_i(1 - p_i) + \\lambda}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## üîÅ Update Step\n",
    "\n",
    "1. Add leaf output to the **log-odds**.\n",
    "2. Convert updated log-odds to **probability**:\n",
    "   $$\n",
    "   P = \\frac{e^{\\text{new log(odds)}}}{1 + e^{\\text{new log(odds)}}}\n",
    "   $$\n",
    "3. Compute new residuals.\n",
    "4. Train next tree on updated residuals.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Assumptions\n",
    "\n",
    "1. **Weak Learners Can Be Combined**:  \n",
    "   Multiple weak models (e.g., shallow trees) can be improved sequentially.\n",
    "\n",
    "2. **Additive Modeling Framework**:  \n",
    "   Improvements from each tree combine additively to improve the model.\n",
    "\n",
    "3. **Target Variable is Predictable**:  \n",
    "   Assumes that meaningful patterns exist in input features.\n",
    "\n",
    "4. **Proper Feature Engineering**:  \n",
    "   Features must be informative and clean (low noise).\n",
    "\n",
    "5. **Low Multicollinearity**:  \n",
    "   High correlation among features may degrade performance.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚ö†Ô∏è Limitations\n",
    "\n",
    "1. **High Computational Demand**  \n",
    "   - Training is slow and memory-intensive for large/high-dimensional data.\n",
    "\n",
    "2. **Risk of Overfitting**  \n",
    "   - Too many deep trees can overfit without proper tuning or regularization.\n",
    "\n",
    "3. **Hyperparameter Complexity**  \n",
    "   - Numerous parameters (learning rate, depth, gamma, etc.) require expert tuning.\n",
    "\n",
    "4. **Less Interpretability**  \n",
    "   - Complex ensemble of trees is harder to explain than single decision trees or logistic regression.\n",
    "\n",
    "5. **Sensitive to Noise**  \n",
    "   - Noisy datasets can mislead the model unless early stopping or regularization is applied.\n",
    "\n",
    "6. **Handling Imbalanced Datasets**  \n",
    "   - Requires special strategies like custom loss functions or class weighting.\n",
    "\n",
    "7. **Scaling Challenges with Sparse Data**  \n",
    "   - Performs better than traditional models on sparse data, but preprocessing is still crucial.\n",
    "\n",
    "8. **Training Complexity**  \n",
    "   - Boosting is sequential in nature, which limits full parallelization (though XGBoost uses optimized techniques like histogram-based training).\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb30e4fb-94b7-4144-a8d3-156cfe3c6ad7",
   "metadata": {},
   "source": [
    "# üîß XGBoost Hyperparameter Guide\n",
    "\n",
    "XGBoost is a powerful and flexible boosting algorithm that provides a wide range of parameters to tune performance for **both classification and regression** tasks. Here's a breakdown of the most important hyperparameters:\n",
    "\n",
    "---\n",
    "\n",
    "## 1Ô∏è‚É£ General Parameters\n",
    "\n",
    "| Parameter   | Description |\n",
    "|-------------|-------------|\n",
    "| `booster`   | Type of model to run: <br>‚Ä¢ `'gbtree'`: tree-based models (default) <br>‚Ä¢ `'gblinear'`: linear models <br>‚Ä¢ `'dart'`: Dropout-based boosting |\n",
    "| `nthread` or `n_jobs` | Number of CPU threads to use for training. Useful for parallel processing. |\n",
    "\n",
    "---\n",
    "\n",
    "## 2Ô∏è‚É£ Booster Parameters (for `'gbtree'` and `'dart'`)\n",
    "\n",
    "These parameters control the trees and boosting behavior.\n",
    "\n",
    "### üå≥ Tree Structure\n",
    "\n",
    "| Parameter         | Description |\n",
    "|------------------|-------------|\n",
    "| `max_depth`       | Maximum depth of a tree. Higher = more complex trees. Prevents underfitting. |\n",
    "| `min_child_weight`| Minimum sum of instance weights needed in a child. Higher values prevent overfitting. |\n",
    "| `gamma`           | Minimum loss reduction required to make a split. Larger gamma = more conservative trees. |\n",
    "| `subsample`       | % of training samples used for growing each tree. Typical values: 0.5 to 1.0 |\n",
    "| `colsample_bytree`| % of features (columns) used per tree. Can reduce overfitting. |\n",
    "| `colsample_bylevel` | % of features used per tree level. |\n",
    "| `colsample_bynode`  | % of features used per split (node). |\n",
    "| `lambda` (reg_lambda) | L2 regularization on weights. Helps with overfitting. |\n",
    "| `alpha` (reg_alpha)   | L1 regularization on weights. Adds sparsity. |\n",
    "\n",
    "---\n",
    "\n",
    "## 3Ô∏è‚É£ Learning Parameters\n",
    "\n",
    "| Parameter     | Description |\n",
    "|---------------|-------------|\n",
    "| `learning_rate` (or `eta`) | Step size shrinkage used in updates. Lower values make learning slower but more robust. Typical range: 0.01‚Äì0.3 |\n",
    "| `n_estimators` | Number of trees (boosting rounds). |\n",
    "| `scale_pos_weight` | Used for imbalanced classification. Ratio of negative to positive classes. |\n",
    "\n",
    "---\n",
    "\n",
    "## 4Ô∏è‚É£ Objective Parameters\n",
    "\n",
    "| Parameter | Description |\n",
    "|-----------|-------------|\n",
    "| `objective` | Learning task type:<br>‚Ä¢ `'binary:logistic'`: binary classification<br>‚Ä¢ `'multi:softmax'`: multiclass classification (with `num_class`)<br>‚Ä¢ `'multi:softprob'`: same as softmax, but returns probabilities<br>‚Ä¢ `'reg:squarederror'`: regression (default)<br>‚Ä¢ `'reg:logistic'`: logistic regression |\n",
    "| `eval_metric` | Evaluation metric:<br>‚Ä¢ Classification: `'logloss'`, `'error'`, `'auc'`<br>‚Ä¢ Regression: `'rmse'`, `'mae'`, `'rmsle'` |\n",
    "\n",
    "---\n",
    "\n",
    "## 5Ô∏è‚É£ Dart-Specific Parameters (Optional)\n",
    "\n",
    "| Parameter | Description |\n",
    "|-----------|-------------|\n",
    "| `sample_type` | Sampling method: `'uniform'` or `'weighted'` |\n",
    "| `normalize_type` | How to normalize dropped trees: `'tree'` or `'forest'` |\n",
    "| `rate_drop` | Dropout rate for trees |\n",
    "| `skip_drop` | Probability of skipping the dropout during iteration |\n",
    "\n",
    "---\n",
    "\n",
    "## üß† Typical Settings\n",
    "\n",
    "### üîµ Binary Classification\n",
    "\n",
    "```python\n",
    "params = {\n",
    "    'objective': 'binary:logistic',\n",
    "    'eval_metric': 'logloss',\n",
    "    'max_depth': 4,\n",
    "    'learning_rate': 0.1,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'scale_pos_weight': 1,\n",
    "    'n_estimators': 100\n",
    "}\n",
    "\n",
    "params = {\n",
    "    'objective': 'reg:squarederror',\n",
    "    'eval_metric': 'rmse',\n",
    "    'max_depth': 4,\n",
    "    'learning_rate': 0.1,\n",
    "    'subsample': 0.9,\n",
    "    'colsample_bytree': 0.9,\n",
    "    'n_estimators': 100\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b687fdcd-72fd-4542-90df-dfa38b16c6aa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
