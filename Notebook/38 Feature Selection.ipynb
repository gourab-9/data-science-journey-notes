{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "462590ad-6368-4074-8ff8-d6bbc7b267b9",
   "metadata": {},
   "source": [
    "# Feature Selection: Picking the Right Inputs for Better Models\r\n",
    "\r\n",
    "In Machine Learning, **features** are the input columns we feed into a model.\r\n",
    "\r\n",
    "**Feature Selection** means identifying and choosing the *most important* features from all available ones, creating a smarter, leaner subset.  \r\n",
    "Not every feature is valuable — sometimes, **less is more**!\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "## Why Perform Feature Selection?\r\n",
    "\r\n",
    "1. **Curse of Dimensionality**  \r\n",
    "   - Models perform best with an optimum number of features.  \r\n",
    "   - Adding too many can introduce **sparsity** and **reduce performance** instead of improving it.\r\n",
    "\r\n",
    "2. **Computational Complexity**  \r\n",
    "   - Fewer features mean faster training and less resource consumption.\r\n",
    "\r\n",
    "3. **Interpretability**  \r\n",
    "   - Models with fewer, relevant features are easier to interpret, explain, and deploy.\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "## Types of Feature Selection Techniques\r\n",
    "\r\n",
    "- **Filter Methods**  \r\n",
    "  (Selection based on statistical properties of the data.)\r\n",
    "\r\n",
    "- **Wrapper Methods**  \r\n",
    "  (Selection based on model performance metrics.)\r\n",
    "\r\n",
    "- **Embedded Methods**  \r\n",
    "  (Feature selection occurs naturally during model training.)\r\n",
    "\r\n",
    "- **Hybrid Methods**  \r\n",
    "  (Combination of filter and wrapper approaches.)\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "## Important Tip: Avoid Data Leakage!\r\n",
    "\r\n",
    "Whenever applying feature selection, **always split your dataset first**:\r\n",
    "- **Training Data** (for model building)\r\n",
    "- **Testing Data** (for model evaluation)\r\n",
    "\r\n",
    "Then, **apply feature selection only on the training data**.\r\n",
    "\r\n",
    "> **Why?**  \r\n",
    "> If you perform feature selection on the entire dataset, the model may accidentally \"see\" parts of the testing data, causing **Data Leakage**.  \r\n",
    "> This results in overly optimistic performance during evaluation, but poor performance in real-world scenarios.\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "✅ **Feature Selection isn't just about reducing features — it's about building smarter, faster, and more reliable models.**\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f1b7d22-fb91-41fd-a8f7-81e311b86465",
   "metadata": {},
   "source": [
    "# Filter-Based Feature Selection Techniques\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "## Techniques\r\n",
    "\r\n",
    "### 1. Drop Duplicate Columns\r\n",
    "- If two columns are identical, drop one.\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "### 2. Variance Threshold\r\n",
    "Variance thresholding is applied mainly on two types of features:\r\n",
    "- **Constant Features:**  \r\n",
    "  Columns where all values are the same (zero variance).\r\n",
    "\r\n",
    "- **Quasi-Constant Features:**  \r\n",
    "  Columns where a large percentage (e.g., 99.5%) of rows have the same value.\r\n",
    "\r\n",
    "**Steps:**\r\n",
    "1. Define a threshold (typically between 0.01 to 0.1).\r\n",
    "2. Calculate the variance for each feature.\r\n",
    "3. Drop the feature if its variance is less than the threshold.\r\n",
    "\r\n",
    "> Use `VarianceThreshold` from `scikit-learn`.\r\n",
    "\r\n",
    "**Points to Consider:**\r\n",
    "- Ignore the target variable.\r\n",
    "- Ignores feature interaction.\r\n",
    "- Sensitive to data scaling.\r\n",
    "- Arbitrary threshold choice.\r\n",
    "\r\n",
    "**Disadvantages:**\r\n",
    "- Assumes linearity.\r\n",
    "- Doesn't capture complex feature relationships.\r\n",
    "- Sensitive to outliers.\r\n",
    "- Threshold selection can be challenging.\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "### 3. ANOVA (Analysis of Variance)\r\n",
    "Use when:\r\n",
    "- **Input Features:** Numerical\r\n",
    "- **Target Variable:** Categorical (with more than two classes)\r\n",
    "\r\n",
    "Variation:\r\n",
    "- If both input and output are numerical, use f-ANOVA.\r\n",
    "\r\n",
    "**Disadvantages:**\r\n",
    "- Assumption of normality.\r\n",
    "- Homogeneity of variance required.\r\n",
    "- Independence of observations needed.\r\n",
    "- Sensitive to outliers.\r\n",
    "- Does not capture feature interactions.\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "### 4. Chi-Square Test\r\n",
    "Use when both input and output columns are categorical.\r\n",
    "\r\n",
    "- Null Hypothesis: No relationship between the two variables.\r\n",
    "- Small p-value → Important feature.\r\n",
    "- Large p-value → Less important feature.\r\n",
    "\r\n",
    "**Disadvantages:**\r\n",
    "- Works only with categorical data.\r\n",
    "- Assumes independence of observations.\r\n",
    "- Requires a sufficient sample size.\r\n",
    "- Ignores interactions between features.\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "## Filter Method: Advantages and Disadvantages\r\n",
    "\r\n",
    "**Advantages:**\r\n",
    "- Simple to apply.\r\n",
    "- Fast and scalable.\r\n",
    "- Suitable as a preprocessing step.\r\n",
    "\r\n",
    "**Disadvantages:**\r\n",
    "- Ignores feature-to-feature relationships.\r\n",
    "- Model-agnostic (does not involve model learning).\r\n",
    "- Limited to basic statistical measures.\r\n",
    "- Arbitrary threshold decisions.\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "## Overall\r\n",
    "\r\n",
    "Filter methods evaluate features independently using statistical tests.  \r\n",
    "However, they ignore possible relationships between features, meaning important combinations might be missed.\r\n",
    "\r\n",
    "---\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f9ff8bb-ab22-45f3-89d1-ae1af5dc9604",
   "metadata": {},
   "source": [
    "# Wrapper Methods for Feature Selection\n",
    "\n",
    "---\n",
    "\n",
    "## What is Wrapper Method?\n",
    "\n",
    "Wrapper methods use a predictive model to score feature subsets.  \n",
    "They are called \"wrapper\" methods because they **wrap** the feature selection process around model evaluation.\n",
    "\n",
    "- All subsets of input features are generated.\n",
    "- Each subset, along with the target variable, is used to train a machine learning model (e.g., Linear Regression).\n",
    "- Model performance (e.g., R²-Score) is calculated for each subset.\n",
    "- The subset with the highest performance score is selected as the best.\n",
    "\n",
    "---\n",
    "\n",
    "## Steps:\n",
    "\n",
    "1. Subset Generation\n",
    "2. Subset Evaluation\n",
    "3. Stopping Criterion\n",
    "\n",
    "---\n",
    "\n",
    "## Four Types of Wrapper Methods:\n",
    "\n",
    "### 1. Exhaustive Feature Selection (Best Subset Selection)\n",
    "- Tries all possible feature combinations.\n",
    "- **Disadvantages:**\n",
    "  - Very high computational complexity.\n",
    "  - Risk of overfitting.\n",
    "  - Requires a good evaluation metric.\n",
    "  \n",
    "- **Note:**  \n",
    "  Gives very good results for small feature sets but impractical when features are large.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Backward Elimination (Sequential Backward Selection)\n",
    "- Starts with all features.\n",
    "- Iteratively removes the least important feature at each step.\n",
    "\n",
    "- **Advantages:**\n",
    "  - Faster than Exhaustive Search.\n",
    "\n",
    "- **Disadvantages:**\n",
    "  - Once a feature is removed, it cannot be re-added.\n",
    "  - May miss globally optimal feature set.\n",
    "\n",
    "- **When to use:**\n",
    "  - Useful when you have many features and want to retain most of them.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Forward Selection (Sequential Forward Selection)\n",
    "- Starts with no features.\n",
    "- Iteratively adds the most important feature at each step.\n",
    "\n",
    "- **Advantages:**\n",
    "  - Good balance between model complexity and performance.\n",
    "\n",
    "- **Disadvantages:**\n",
    "  - Like backward selection, may miss the global best subset.\n",
    "\n",
    "- **When to use:**\n",
    "  - Useful when you want to select a small number of best features from a large feature set.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Recursive Feature Elimination (RFE)\n",
    "- Recursively removes features based on model weights or importance.\n",
    "- A popular method for model-driven feature selection.\n",
    "\n",
    "---\n",
    "\n",
    "## Important Parameters:\n",
    "\n",
    "| Method        | Steps (Approx.)         |\n",
    "|---------------|--------------------------|\n",
    "| Exhaustive    | 2^n - 1 combinations      |\n",
    "| Backward      | n(n+1)/2 steps             |\n",
    "| Forward       | n(n+1)/2 steps             |\n",
    "\n",
    "> Example (Forward Selection with Linear Regression):  \n",
    "> `SFS(lr, k_features='best', forward=True, floating=False, scoring='r2', cv=5)`\n",
    "\n",
    "---\n",
    "\n",
    "## How to decide Forward or Backward?\n",
    "\n",
    "- If you have many features and want to retain most → **Backward Selection**.\n",
    "- If you have many features but want to pick few best → **Forward Selection**.\n",
    "- If you just want \"best features\" without fixed count → Either Forward or Backward.\n",
    "\n",
    "---\n",
    "\n",
    "## Advantages of Wrapper Methods:\n",
    "\n",
    "- High Accuracy.\n",
    "- Considers Feature Interactions.\n",
    "\n",
    "---\n",
    "\n",
    "## Disadvantages of Wrapper Methods:\n",
    "\n",
    "- Very high computational cost.\n",
    "- Risk of Overfitting.\n",
    "- Model-specific (depends on chosen algorithm).\n",
    "\n",
    "---\n",
    "\n",
    "## Filter vs Wrapper Method:\n",
    "\n",
    "| Filter Method                  | Wrapper Method                   |\n",
    "|---------------------------------|-----------------------------------|\n",
    "| Evaluates features individually | Evaluates feature subsets together |\n",
    "| Ignores feature interactions     | Considers feature interactions    |\n",
    "| Fast and simple                 | Computationally expensive         |\n",
    "| Model-agnostic                  | Model-specific                    |\n",
    "\n",
    "---\n",
    "\n",
    "## Overall\n",
    "\n",
    "Wrapper methods study the interaction between features but are computationally expensive and slower compared to filter methods.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b66ba6b1-dffd-49f0-a711-9b4b3d677d9c",
   "metadata": {},
   "source": [
    "# Embedded Method for Feature Selection\n",
    "\n",
    "---\n",
    "\n",
    "## What is Embedded Method?\n",
    "\n",
    "Embedded methods handle feature interactions and are faster than wrapper methods.  \n",
    "They integrate feature selection as part of the model-building process. When you send data to the machine learning model for training, you not only get the model predictions but also the **importance of the features**.\n",
    "\n",
    "### How It Works:\n",
    "- During model training, feature importance is calculated along with the model's prediction.\n",
    "- This approach studies feature interactions and is much faster than wrapper methods.\n",
    "\n",
    "**Applicable Algorithms:**\n",
    "\n",
    "- **Models with coefficients or feature importance attributes:**\n",
    "  - **Linear Models:**  \n",
    "    - Linear Regression, Logistic Regression, Ridge, Lasso, Elastic Net\n",
    "  - **Tree-based Models:**  \n",
    "    - Decision Tree, Random Forest, Gradient Boosting\n",
    "\n",
    "---\n",
    "\n",
    "## Linear Regression Assumptions:\n",
    "\n",
    "1. Linearity\n",
    "2. Independence of errors\n",
    "3. Homoscedasticity (constant variance)\n",
    "4. Normality of errors\n",
    "5. No multicollinearity\n",
    "\n",
    "---\n",
    "\n",
    "## Regularized Linear Models\n",
    "\n",
    "Regularization helps **reduce overfitting** by adding a penalty term to the loss function.  \n",
    "This term discourages complex models and lowers the coefficients, which reduces the model's overfitting tendency.\n",
    "\n",
    "**Types of Regularization:**\n",
    "- **Ridge:** Reduces coefficients but does not set them to zero.\n",
    "- **Lasso:** Can completely set some feature coefficients to zero (great for feature selection).\n",
    "- **Elastic Net:** A mix of Ridge and Lasso.\n",
    "\n",
    "---\n",
    "\n",
    "## Tree-based Models\n",
    "\n",
    "- **Decision Tree**, **Random Forest**, **Gradient Boosting**:  \n",
    "  All tree-based algorithms naturally provide feature importance.\n",
    "\n",
    "---\n",
    "\n",
    "## Recursive Feature Elimination (RFE)\n",
    "\n",
    "- **How It Works:**\n",
    "  1. Use a model that provides coefficients or feature importance.\n",
    "  2. Rank features by importance.\n",
    "  3. Iteratively remove the least important features until the best feature subset is selected.\n",
    "\n",
    "- **Hybrid Approach:**  \n",
    "  Combines both **Embedded** and **Wrapper** methods for enhanced feature selection.\n",
    "\n",
    "---\n",
    "\n",
    "## Advantages of Embedded Methods:\n",
    "\n",
    "1. **High Performance:** Efficient and fast.\n",
    "2. **Less Prone to Overfitting:** As feature selection is embedded in the model.\n",
    "3. **Efficiency:** Integrates feature selection and model building.\n",
    "\n",
    "---\n",
    "\n",
    "## Disadvantages of Embedded Methods:\n",
    "\n",
    "1. **Model-Specific:** Dependent on the chosen algorithm.\n",
    "2. **Complexity:** More complex than filter-based methods.\n",
    "3. **Requires Tuning:** Models need proper tuning.\n",
    "4. **Stability:** Can be sensitive to data fluctuations.\n",
    "\n",
    "---\n",
    "\n",
    "## Filter-based Method: Mutual Information\n",
    "\n",
    "**What it does:** Measures the dependency between two columns (features).  \n",
    "It calculates how much information is shared between them.\n",
    "\n",
    "- **Formula:**  \n",
    "  $$\n",
    "  MI(X, Y) = \\sum P(X, Y) \\log \\left( \\frac{P(X, Y)}{P(X) P(Y)} \\right)\n",
    "  $$\n",
    "\n",
    "- **Advantages:**\n",
    "  1. Non-negative.\n",
    "  2. Symmetric.\n",
    "  3. Can capture any type of statistical dependency.\n",
    "\n",
    "- **Disadvantages:**\n",
    "  1. Estimation difficulty.\n",
    "  2. Requires large sample sizes.\n",
    "  3. Computationally expensive.\n",
    "  4. Difficulty with continuous variables.\n",
    "  5. Doesn’t indicate the nature of relationships.\n",
    "  6. Doesn’t account for redundancy.\n",
    "\n",
    "---\n",
    "\n",
    "## Overall\n",
    "\n",
    "**Embedded methods** are a good choice for handling feature selection with minimal computational cost compared to wrapper methods while accounting for feature interactions.  \n",
    "But they are model-specific and may require more complex tuning and stability considerations.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d765bc3d-5edb-4840-b4ea-e7a72111e48b",
   "metadata": {},
   "source": [
    "# Feature Selection Cheat Sheet\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "## 1) Filter Methods:\r\n",
    "- **Variance Threshold:** Removes features with low variance.\r\n",
    "- **Correlation Coefficient:** Removes features highly correlated with others.\r\n",
    "- **Chi-Square Test:** Measures the relationship between categorical features and target.\r\n",
    "- **Mutual Information:** Captures dependency between features.\r\n",
    "- **ANOVA:** Measures the relationship between numerical input and categorical output.\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "## 2) Wrapper Methods:\r\n",
    "- **Recursive Feature Elimination (RFE):** Iteratively removes features to find the optimal subset.\r\n",
    "- **Sequential Feature Selection (SFS):** Adds or removes features one by one based on model performance.\r\n",
    "- **Exhaustive Feature Selection:** Tries all possible feature subsets to find the best combination.\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "## 3) Embedded Methods:\r\n",
    "- **Lasso Regression:** Uses L1 regularization to shrink coefficients and perform feature selection.\r\n",
    "- **Ridge Regression:** Uses L2 regularization to reduce feature impact.\r\n",
    "- **Elastic Net:** Combines both L1 and L2 regularization.\r\n",
    "- **Random Forest Feature Importance:** Measures feature importance based on tree-based models.\r\n",
    "\r\n",
    "---\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6fd6072-5e54-425d-85be-0d4166886fa2",
   "metadata": {},
   "source": [
    "# Code Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "33e0b046-f5f7-43e1-96c0-535429130a0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original X shape: (4, 4)\n",
      "New X shape after variance thresholding: (4, 3)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold\n",
    "import numpy as np\n",
    "\n",
    "# Example: Removing features with low variance\n",
    "# X is your input feature matrix, where each row is a sample and each column is a feature\n",
    "# y is the target variable\n",
    "\n",
    "# Simulate some example data (for illustration purposes)\n",
    "X = np.array([[1, 2, 3, 1],   # Feature 1: High variance\n",
    "              [1, 2, 3, 1],   # Feature 2: High variance\n",
    "              [1, 2, 3, 1],   # Feature 3: Low variance\n",
    "              [4, 5, 6, 1]])  # Feature 4: High variance\n",
    "y = np.array([0, 1, 0, 1])\n",
    "\n",
    "# Apply VarianceThreshold to remove features with variance below 0.1\n",
    "selector = VarianceThreshold(threshold=0.1)\n",
    "X_new = selector.fit_transform(X)\n",
    "\n",
    "print(\"Original X shape:\", X.shape)\n",
    "print(\"New X shape after variance thresholding:\", X_new.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e75474eb-4e48-4ff1-967f-6b45e800d346",
   "metadata": {},
   "source": [
    "# Correlation Coefficient\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "\n",
    "# Example: Remove highly correlated features\n",
    "corr_matrix = df.corr().abs()\n",
    "upper_triangle = corr_matrix.where(\n",
    "    pd.np.triu(pd.np.ones(corr_matrix.shape), k=1).astype(bool)\n",
    ")\n",
    "to_drop = [column for column in upper_triangle.columns if any(upper_triangle[column] > 0.9)]\n",
    "df.drop(columns=to_drop, inplace=True)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "700f8c28-1411-4c26-82c4-96c2c036b4a3",
   "metadata": {},
   "source": [
    "# Chi-Square Test\n",
    "```python \n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "\n",
    "# Example: Applying Chi-Square test for feature selection\n",
    "X_new = SelectKBest(chi2, k=5).fit_transform(X, y)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e5b9c2-5191-4faf-9d65-b4a0dc79129e",
   "metadata": {},
   "source": [
    "# Mutual Information\n",
    "```python\n",
    "from sklearn.feature_selection import SelectKBest\r\n",
    "from sklearn.feature_selection import mutual_info_classif\r\n",
    "\r\n",
    "# Example: Applying Mutual Information for feature selection\r\n",
    "X_new = SelectKBest(mutual_info_classif, k=5).fit_transform(X,\n",
    "``` y)\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f32d7c-189a-4d35-bc48-ad6469c4c371",
   "metadata": {},
   "source": [
    "# ANOVA\n",
    "``` Python\n",
    "from sklearn.feature_selection import f_classif\r\n",
    "from sklearn.feature_selection import SelectKBest\r\n",
    "\r\n",
    "# Example: Applying ANOVA F-test for feature selection\r\n",
    "X_new = SelectKBest(f_classif, k=5).fit_transform(X,``` y)\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab48edc1-9804-4f2f-9bab-2a653014d307",
   "metadata": {},
   "source": [
    "# 2) Wrapper Methods:\n",
    "### Recursive Feature Elimination (RFE)\n",
    "```python\n",
    "from sklearn.feature_selection import RFE\r\n",
    "from sklearn.linear_model import LogisticRegression\r\n",
    "\r\n",
    "# Example: Recursive Feature Elimination\r\n",
    "model = LogisticRegression()\r\n",
    "rfe = RFE(model, 5)  # Select 5 features\r\n",
    "X_new = rfe.fit_transform(\n",
    "```X, y)\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac1fef0-3803-4a48-9b97-0a4385a023e4",
   "metadata": {},
   "source": [
    "# Sequential Feature Selection (SFS)\n",
    "```python\n",
    "from sklearn.feature_selection import SequentialFeatureSelector\r\n",
    "from sklearn.linear_model import LinearRegression\r\n",
    "\r\n",
    "# Example: Sequential Forward Selection\r\n",
    "model = LinearRegression()\r\n",
    "sfs = SequentialFeatureSelector(model, n_features_to_select=5)\r\n",
    "sfs.fit(X, y)\r\n",
    "X_new = sfs.transf\n",
    "```orm(X)\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddcd737b-53eb-44b3-a650-1e54623a1cc8",
   "metadata": {},
   "source": [
    "# Exhaustive Feature Selection\n",
    "```python\n",
    "from mlxtend.feature_selection import ExhaustiveFeatureSelector\r\n",
    "from sklearn.linear_model import LogisticRegression\r\n",
    "\r\n",
    "# Example: Exhaustive Feature Selection\r\n",
    "model = LogisticRegression()\r\n",
    "efs = ExhaustiveFeatureSelector(model, min_features=1, max_features=5, scoring='accuracy')\r\n",
    "efs.fit(X, y)\r\n",
    "X_new = efs.transf\n",
    "```orm(X)\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f85cf7e-6961-417e-adae-c15b2266db7e",
   "metadata": {},
   "source": [
    "# 3) Embedded Methods:\n",
    "### Lasso Regression\n",
    "```python \n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "# Example: Lasso for feature selection\n",
    "lasso = Lasso(alpha=0.1)\n",
    "lasso.fit(X, y)\n",
    "X_new = X[:, lasso.coef_ != 0]  # Select features with non-zero coefficients\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a23f5db0-4a7c-4072-8d39-3d2e4fab2e56",
   "metadata": {},
   "source": [
    "# Ridge Regression\n",
    "```python\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "# Example: Ridge for feature selection\n",
    "ridge = Ridge(alpha=0.1)\n",
    "ridge.fit(X, y)\n",
    "X_new = X[:, ridge.coef_ != 0]  # Select features with non-zero coefficients\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c690557-ef5e-4bab-9d21-74590f029c44",
   "metadata": {},
   "source": [
    "# Elastic Net\n",
    "```python\n",
    "from sklearn.linear_model import ElasticNet\r\n",
    "\r\n",
    "# Example: Elastic Net for feature selection\r\n",
    "elastic_net = ElasticNet(alpha=0.1, l1_ratio=0.5)\r\n",
    "elastic_net.fit(X, y)\r\n",
    "X_new = X[:, elastic_net.coef_ != 0]  # Select features with non-zero coeffici```ents\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45f324e4-481e-43d6-8357-2e1b7081ef2c",
   "metadata": {},
   "source": [
    "# Random Forest Feature Importance\n",
    "```python\n",
    "from sklearn.ensemble import RandomForestClassifier\r\n",
    "\r\n",
    "# Example: Using Random Forest for feature importance\r\n",
    "model = RandomForestClassifier()\r\n",
    "model.fit(X, y)\r\n",
    "importances = model.feature_importances_\r\n",
    "\r\n",
    "# Select top features\r\n",
    "indices = importances.argsort()[-5:][::-1]  # Top 5 important features\r\n",
    "X_new = X[:, ```indices]\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a2546b-f3cd-4d06-a753-03458aba71dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
