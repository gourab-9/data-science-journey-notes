{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b56d38c2-9c73-45d7-8a36-bdff839f22bb",
   "metadata": {},
   "source": [
    "# üß† ColumnTransformer in Scikit-learn\n",
    "\n",
    "## üìå Definition\n",
    "\n",
    "`ColumnTransformer` is a class in `scikit-learn` that allows you to apply **different preprocessing techniques** to **specific columns** in your dataset. It is especially useful when working with datasets that include a **mix of numerical and categorical features**, each of which may require a different kind of transformation.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Why Use ColumnTransformer?\n",
    "\n",
    "When handling real-world datasets, you may need to:\n",
    "\n",
    "- Impute or scale **numerical columns**\n",
    "- Encode **categorical columns**\n",
    "- Leave some columns unchanged\n",
    "- Drop unnecessary columns\n",
    "\n",
    "Doing this manually can be tedious and error-prone. `ColumnTransformer` handles this cleanly in **one unified step**.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚öôÔ∏è How ColumnTransformer Works\n",
    "\n",
    "1. You pass a **list of transformers** as tuples:  \n",
    "   ```python\n",
    "   (name, transformer, columns)\n",
    "\n",
    "    ```\n",
    "\n",
    "- **name**: A string label for the transformer.\n",
    "- **transformer**: The preprocessing object (e.g., `SimpleImputer`, `StandardScaler`, `OneHotEncoder`).\n",
    "- **columns**: List of column names or indices to apply the transformer to.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "2. `ColumnTransformer` applies each transformer only to the specified columns.\n",
    "3. It then **concatenates all transformed columns** into a single output.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b19baa2d-99f6-49be-88d3-b8fbf57b0fd8",
   "metadata": {},
   "source": [
    "# üß™ Code Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5e56925f-5a0e-4ddc-bdd6-6273d1928345",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "\n",
    "# Sample data\n",
    "df = pd.DataFrame({\n",
    "    'Age': [25, 30, None, 22],\n",
    "    'Gender': ['M', 'F', 'F', 'M'],\n",
    "    'Salary': [50000, 60000, 52000, None]\n",
    "})\n",
    "\n",
    "# Define transformer\n",
    "ct = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', SimpleImputer(strategy='mean'), ['Age', 'Salary']),\n",
    "        ('cat', OneHotEncoder(), ['Gender'])\n",
    "    ],\n",
    "    remainder='drop'  # what to do with columns not listed\n",
    ")\n",
    "\n",
    "# Apply transformation\n",
    "    X_transformed = ct.fit_transform(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "30e422cc-e673-4544-8d47-c01f1f646b2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.50000000e+01, 5.00000000e+04, 0.00000000e+00, 1.00000000e+00],\n",
       "       [3.00000000e+01, 6.00000000e+04, 1.00000000e+00, 0.00000000e+00],\n",
       "       [2.56666667e+01, 5.20000000e+04, 1.00000000e+00, 0.00000000e+00],\n",
       "       [2.20000000e+01, 5.40000000e+04, 0.00000000e+00, 1.00000000e+00]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_transformed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab55b210-e86c-4c75-9ca5-f14b1ea8cf0b",
   "metadata": {},
   "source": [
    "### üîç Output Explanation\n",
    "\n",
    "- `SimpleImputer` is applied to **Age** and **Salary**.\n",
    "- `OneHotEncoder` is applied to **Gender**.\n",
    "\n",
    "---\n",
    "\n",
    "If other columns existed:\n",
    "\n",
    "- `'drop'` would **discard** them.\n",
    "- `'passthrough'` would **keep** them as-is in the final output.\n",
    "\n",
    "## ‚öôÔ∏è Parameters of ColumnTransformer\r\n",
    "\r\n",
    "| Parameter            | Description |\r\n",
    "|----------------------|-------------|\r\n",
    "| **transformers**      | A list of `(name, transformer, columns)` tuples |\r\n",
    "| **remainder**         | What to do with columns not listed in transformers:<br>‚Ä¢ `'drop'` (default): remove them<br>‚Ä¢ `'passthrough'`: include them unchanged |\r\n",
    "| **sparse_threshold**  | If the result is sparse and % of non-zeros is below this, output is sparse |\r\n",
    "| **n_jobs**            | Number of jobs to run in parallel (e.g., `-1` for all CPUs) |\r\n",
    "| **transformer_weights** | Optional weighting of individual transfo\n",
    "\n",
    "## üîÑ remainder='drop' vs 'passthrough'\r\n",
    "\r\n",
    "| Option          | Behavior |\r\n",
    "|-----------------|----------|\r\n",
    "| `'drop'`        | Columns not listed are **removed** from the output |\r\n",
    "| `'passthrough'` | Columns not listed are **added** to the final output **without transformation** |\r\n",
    "rmers |\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a213bc6a-107b-428c-a854-70ce310180c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example with passthrough\n",
    "ct = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', SimpleImputer(strategy='mean'), ['Age']),\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a83613-7d1b-4511-862c-d48908278925",
   "metadata": {},
   "source": [
    "## üìå Summary\n",
    "\n",
    "- `ColumnTransformer` makes preprocessing **clean** and **modular**.\n",
    "- Use it when your dataset contains **different feature types**.\n",
    "- Combine it with a `Pipeline` to build **end-to-end ML workflows**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "693908fa-98b1-4d2f-ad33-16fb425afabb",
   "metadata": {},
   "source": [
    "# Scikit-learn: `make_column_transformer`\n",
    "\n",
    "## Definition\n",
    "\n",
    "`make_column_transformer` is a helper function in Scikit-learn that simplifies the creation of a `ColumnTransformer`. It provides a quick way to apply different preprocessing steps (like scaling, encoding, etc.) to different subsets of the feature columns in a dataset.\n",
    "\n",
    "## Difference Between `make_column_transformer` and `ColumnTransformer`\n",
    "\n",
    "- **`ColumnTransformer`**: A general class in Scikit-learn used to apply different preprocessing operations to different subsets of columns. You specify the transformers explicitly along with the column indices or names.\n",
    "  \n",
    "- **`make_column_transformer`**: A convenient function that simplifies the process of creating a `ColumnTransformer`. It allows you to specify transformations directly in a simpler way.\n",
    "\n",
    "**Key difference**: `make_column_transformer` is a shorthand that automatically creates a `ColumnTransformer` based on the passed transformers, making it more concise.\n",
    "\n",
    "## Working\n",
    "\n",
    "1. **Specify Columns**: You can specify which columns should receive a particular transformation.\n",
    "2. **Apply Transformation**: The transformer or preprocessor (e.g., `StandardScaler`, `OneHotEncoder`, etc.) is applied to the specified columns.\n",
    "3. **Handle Non-Specified Columns**: You can set the `remainder` argument to either `drop` or `passthrough` to determine what happens to the columns that aren't explicitly transformed.\n",
    "\n",
    "## Code Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ee88cc99-066f-45f0-8177-8939166ded5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 0)\t-0.9006811702978088\n",
      "  (0, 1)\t1.019004351971607\n",
      "  (0, 6)\t1.0\n",
      "  (0, 46)\t1.0\n",
      "  (1, 0)\t-1.1430169111851105\n",
      "  (1, 1)\t-0.13197947932162468\n",
      "  (1, 6)\t1.0\n",
      "  (1, 46)\t1.0\n",
      "  (2, 0)\t-1.3853526520724133\n",
      "  (2, 1)\t0.32841405319566835\n",
      "  (2, 5)\t1.0\n",
      "  (2, 46)\t1.0\n",
      "  (3, 0)\t-1.5065205225160652\n",
      "  (3, 1)\t0.09821728693702184\n",
      "  (3, 7)\t1.0\n",
      "  (3, 46)\t1.0\n",
      "  (4, 0)\t-1.0218490407414595\n",
      "  (4, 1)\t1.2492011182302534\n",
      "  (4, 6)\t1.0\n",
      "  (4, 46)\t1.0\n",
      "  (5, 0)\t-0.537177558966854\n",
      "  (5, 1)\t1.939791417006192\n",
      "  (5, 9)\t1.0\n",
      "  (5, 48)\t1.0\n",
      "  (6, 0)\t-1.5065205225160652\n",
      "  :\t:\n",
      "  (143, 64)\t1.0\n",
      "  (144, 0)\t1.0380047568006125\n",
      "  (144, 1)\t0.5586108194543139\n",
      "  (144, 35)\t1.0\n",
      "  (144, 66)\t1.0\n",
      "  (145, 0)\t1.0380047568006125\n",
      "  (145, 1)\t-0.13197947932162468\n",
      "  (145, 30)\t1.0\n",
      "  (145, 64)\t1.0\n",
      "  (146, 0)\t0.5533332750260068\n",
      "  (146, 1)\t-1.2829633106148564\n",
      "  (146, 28)\t1.0\n",
      "  (146, 60)\t1.0\n",
      "  (147, 0)\t0.7956690159133096\n",
      "  (147, 1)\t-0.13197947932162468\n",
      "  (147, 30)\t1.0\n",
      "  (147, 61)\t1.0\n",
      "  (148, 0)\t0.432165404582356\n",
      "  (148, 1)\t0.7888075857129604\n",
      "  (148, 32)\t1.0\n",
      "  (148, 64)\t1.0\n",
      "  (149, 0)\t0.06866179325140237\n",
      "  (149, 1)\t-0.13197947932162468\n",
      "  (149, 29)\t1.0\n",
      "  (149, 59)\t1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load a sample dataset\n",
    "data = load_iris()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Define the column transformer using make_column_transformer\n",
    "column_transformer = make_column_transformer(\n",
    "    (StandardScaler(), [0, 1]),  # Scale the first two columns (features 0 and 1)\n",
    "    (OneHotEncoder(), [2, 3]),   # One hot encode the last two columns (features 2 and 3)\n",
    "    remainder='passthrough'      # Leave the rest of the columns as they are\n",
    ")\n",
    "\n",
    "# Apply the transformations\n",
    "X_transformed = column_transformer.fit_transform(X)\n",
    "\n",
    "# Check the transformed data\n",
    "print(X_transformed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c5678a-0e63-4cf7-9989-ffd4230c4267",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è Explanation of Parameters\n",
    "\n",
    "- **Transformers**: The transformations you want to apply to the columns (e.g., `StandardScaler()`, `OneHotEncoder()`).\n",
    "\n",
    "- **Columns**: The columns to which the respective transformer should be applied. This can be:\n",
    "  - A list of column **indices**.\n",
    "  - A list of column **names**.\n",
    "  - A **slice** object.\n",
    "\n",
    "- **remainder**: This parameter determines what happens to the columns that are not explicitly transformed:\n",
    "  - `remainder='drop'`: Drop the columns that are not specified in the transformer.\n",
    "  - `remainder='passthrough'`: Leave the columns unchanged and pass them through the transformer as they are.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚ö° Explanation of `remainder` Values\n",
    "\n",
    "1. **`remainder='drop'`**: Any columns not explicitly selected for transformation will be **dropped** from the final result.\n",
    "\n",
    "---\n",
    "\n",
    "### Example\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "79c27e9f-fe01-4186-a62f-ab7934556c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "column_transformer = make_column_transformer(\n",
    "    (StandardScaler(), [0, 1]),\n",
    "    (OneHotEncoder(), [2, 3]),\n",
    "    remainder='drop'  # Drop all other columns\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef7bc8d7-2de2-48db-85cf-9eee6ea986f4",
   "metadata": {},
   "source": [
    "2.  ‚ö° Explanation of `remainder='passthrough'`\n",
    "\n",
    "- **`remainder='passthrough'`**: Any columns not explicitly selected for transformation will be **passed through without change**, meaning they will appear as they are in the final transformed dataset.\n",
    "\n",
    "---\n",
    "\n",
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a81906d5-e2f8-4f21-9331-55d7737dc604",
   "metadata": {},
   "outputs": [],
   "source": [
    "column_transformer = make_column_transformer(\n",
    "    (StandardScaler(), [0, 1]),\n",
    "    (OneHotEncoder(), [2, 3]),\n",
    "    remainder='passthrough'  # Keep all other columns unchanged\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b540d7b7-0957-4115-b2d4-19cea3477f68",
   "metadata": {},
   "source": [
    "## üìä Use Case\n",
    "\n",
    "`make_column_transformer` is particularly useful when working with datasets that have **different types of columns** (numerical, categorical, etc.), as it allows you to easily apply different preprocessing steps to different columns in a **clean**, **efficient** manner.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4ff45de-4f7a-464e-9c6a-38c1fcf858b1",
   "metadata": {},
   "source": [
    "## üîÑ Comparison: `ColumnTransformer` vs `make_column_transformer`\n",
    "\n",
    "| **Feature**                     | **ColumnTransformer** | **make_column_transformer** |\n",
    "|----------------------------------|------------------------|-----------------------------|\n",
    "| **Type**                         | Class                 | Function                    |\n",
    "| **Requires naming transformers** | ‚úÖ Yes                | ‚ùå No (auto-names them)     |\n",
    "| **Verbose**                      | More explicit         | More concise                |\n",
    "| **Use Case**                     | Complex pipelines     | Simpler or quick pipelines  |\n",
    "| **Custom step names**            | Yes                   | No                          |\n",
    "\n",
    "---\n",
    "\n",
    "## ‚ùì When to Use Which?\n",
    "\n",
    "| **Situation**                              | **Recommended Tool**        |\n",
    "|---------------------------------------------|-----------------------------|\n",
    "| You want to name each step clearly          | `ColumnTransformer`         |\n",
    "| You want to write quick code                | `make_column_transformer`   |\n",
    "| You're using a complex pipeline             | `ColumnTransformer`         |\n",
    "| You're just experimenting                   | `make_column_transformer`   |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "845cad3b-c1cc-44da-904a-74ac623db30c",
   "metadata": {},
   "source": [
    "# Scikit-learn: **Pipeline in Machine Learning**\n",
    "\n",
    "## Definition\n",
    "\n",
    "A **Pipeline** in machine learning is a way to streamline the process of applying various transformations and model training steps into a single object. It allows for the bundling of several steps such as:\n",
    "- Preprocessing (e.g., scaling, encoding, imputation)\n",
    "- Feature selection\n",
    "- Model fitting and prediction\n",
    "\n",
    "This is particularly useful when there are multiple stages to your data processing and you want to ensure all steps are applied consistently, and it also helps avoid data leakage.\n",
    "\n",
    "## Need for Pipeline\n",
    "\n",
    "- **Consistency**: Applying the same transformations to both the training and test sets.\n",
    "- **Simplifies Code**: Organizes and simplifies the machine learning workflow into a series of well-defined steps.\n",
    "- **Avoid Data Leakage**: Ensures that transformations like scaling or feature selection are fit on the training data and not on the test data.\n",
    "- **Hyperparameter Tuning**: Pipelines are useful when doing cross-validation or grid search over hyperparameters since the whole process is encapsulated in one object.\n",
    "\n",
    "## Flow Diagram\n",
    "\n",
    "```plaintext\n",
    "+---------------------+\n",
    "|  Raw Data (Dataset) |\n",
    "+---------------------+\n",
    "           |\n",
    "           v\n",
    "+-------------------------+\n",
    "|   Step 1: Imputation    |    --->  Missing value handling\n",
    "+-------------------------+\n",
    "           |\n",
    "           v\n",
    "+--------------------------+\n",
    "|   Step 2: Scaling        |    --->  Scaling features (e.g., MinMaxScaler)\n",
    "+--------------------------+\n",
    "           |\n",
    "           v\n",
    "+---------------------------+\n",
    "|   Step 3: Feature Selection|    --->  Feature selection using methods like chi-squared test\n",
    "+---------------------------+\n",
    "           |\n",
    "           v\n",
    "+----------------------+\n",
    "|   Step 4: Model      |    --->  Fit a machine learning model\n",
    "|   Training & Fitting |\n",
    "+----------------------+\n",
    "           |\n",
    "           v\n",
    "+-----------------------+\n",
    "|   Step 5: Prediction  |    --->  Make predictions on new data\n",
    "+-----------------------+\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a214827c-91b9-48fb-8d45-e34af897cd60",
   "metadata": {},
   "source": [
    "## üîß Code Example with Multiple Steps\n",
    "\n",
    "Let‚Äôs define a `Pipeline` with several steps, including column transformation, scaling, feature selection, and model training.\n",
    "\n",
    "### Step-by-Step Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8099b6f9-2cf0-4606-b210-655d656ccd09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Score:  0.9666666666666667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\goura\\anaconda3\\Lib\\site-packages\\sklearn\\feature_selection\\_univariate_selection.py:776: UserWarning: k=5 is greater than n_features=1. All the features will be returned.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Load a dataset (e.g., Iris dataset)\n",
    "data = load_iris()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Step 1: ColumnTransformer with SimpleImputer\n",
    "trf1 = ColumnTransformer([\n",
    "    ('imputer', SimpleImputer(strategy='mean'), [2])  # Impute missing values in column 2\n",
    "])\n",
    "\n",
    "# Step 2: Scaling using MinMaxScaler on first 8 features (for demonstration, we'll assume there are 8 features)\n",
    "trf2 = ColumnTransformer([\n",
    "    ('scaler', MinMaxScaler(), slice(0, 8))  # Scale features from index 0 to 7\n",
    "])\n",
    "\n",
    "# Step 3: Feature Selection using chi2 (select top 5 features)\n",
    "trf3 = SelectKBest(chi2, k=5)\n",
    "\n",
    "# Step 4: Model Training (e.g., Logistic Regression)\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "\n",
    "# Combine all steps into a single pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('imputer', trf1),       # Step 1: Imputation\n",
    "    ('scaler', trf2),        # Step 2: Scaling\n",
    "    ('feature_selection', trf3),  # Step 3: Feature selection\n",
    "    ('model', model)         # Step 4: Model training\n",
    "])\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Fit the pipeline\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Test Score: \", pipeline.score(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad89000b-b44c-480e-9822-8dc8fbe64438",
   "metadata": {},
   "source": [
    "## üõ†Ô∏è Explanation of Steps in the Pipeline\n",
    "\n",
    "### 1. Imputation (`trf1`)\n",
    "\n",
    "- **Purpose**: Handle missing values in the dataset.\n",
    "- **Transformer Used**: `SimpleImputer`\n",
    "- **Details**:  \n",
    "  We use `SimpleImputer` to fill missing values in specific columns.  \n",
    "  In this example, we are filling missing values in **column index 2**.\n",
    "\n",
    "---\n",
    "\n",
    "### Code Example:\n",
    "```Python\n",
    "trf1 = ColumnTransformer([('imputer', SimpleImputer(strategy='mean'), [2])])\n",
    "```\n",
    "---\n",
    "---\n",
    "### 2. Scaling (`trf2`)\n",
    "\n",
    "- **Purpose**: Scale features to a specific range (e.g., [0, 1]).\n",
    "- **Transformer Used**: `MinMaxScaler`\n",
    "- **Details**:  \n",
    "  We apply `MinMaxScaler` to scale all features from **index 0 to 7** (for example).\n",
    "\n",
    "---\n",
    "\n",
    "### Code Example:\n",
    "```python\n",
    "trf2 = ColumnTransformer([('scaler', MinMaxScaler(), slice(0, 8))])\n",
    "```\n",
    "---\n",
    "---\n",
    "### 3. Feature Selection (`trf3`)\n",
    "\n",
    "- **Purpose**: Select the most relevant features.\n",
    "- **Transformer Used**: `SelectKBest`\n",
    "- **Details**:  \n",
    "  We use `SelectKBest` with the chi-squared test (`chi2`) to select the **top 5 features**.\n",
    "\n",
    "---\n",
    "\n",
    "### Code Example:\n",
    "```python\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "\n",
    "trf3 = SelectKBest(chi2, k=5)\n",
    "```\n",
    "---\n",
    "---\n",
    "### 4. Model Training\n",
    "\n",
    "- **Purpose**: Train the final classification model.\n",
    "- **Model Used**: `LogisticRegression`\n",
    "- **Details**:  \n",
    "  Finally, we fit a `LogisticRegression` model.\n",
    "\n",
    "---\n",
    "\n",
    "### Code Example:\n",
    "```python\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "```\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e78f5f0c-6b38-4c05-adda-0fe068e65ab5",
   "metadata": {},
   "source": [
    "## üìå Key Points\n",
    "\n",
    "- **Pipeline** combines all steps in a sequence.\n",
    "- Each step in the pipeline is defined by a **name** and the corresponding **transformer or estimator** (e.g., `SimpleImputer`, `MinMaxScaler`).\n",
    "- **Hyperparameter tuning** can be easily done across the entire pipeline, ensuring that each step is treated as part of the overall model-building process.\n",
    "- **ColumnTransformer** allows you to apply different transformations to different subsets of features, and it can be used as a step in the pipeline.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca399ff-c0f2-42ca-9ee6-80361be5c550",
   "metadata": {},
   "source": [
    "## üß© How to Call Columns by Index or Name\n",
    "\n",
    "- **By Index**: You can use column indices directly, like `[2]` for the 3rd column.\n",
    "- **By Name**: You can use column names, such as `['feature_name']`, but the dataset must have named columns (such as with `pandas.DataFrame`).\n",
    "\n",
    "---\n",
    "\n",
    "### Code Examples:\n",
    "\n",
    "```python\n",
    "# By index\n",
    "ColumnTransformer([('imputer', SimpleImputer(), [2])])\n",
    "\n",
    "# By name (if using pandas DataFrame)\n",
    "ColumnTransformer([('imputer', SimpleImputer(), ['feature_name'])])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "937ac050-8f71-4d0c-9926-97c917480b0f",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è Parameters Explanation in the Pipeline\n",
    "\n",
    "- **ColumnTransformer**:  \n",
    "  Used for selecting and transforming subsets of columns. You can pass the names of transformers, the specific columns to transform, and the transformation to apply.\n",
    "\n",
    "- **SimpleImputer**:  \n",
    "  Used to handle missing data by replacing missing values with a specified strategy (e.g., mean, median).\n",
    "\n",
    "- **MinMaxScaler**:  \n",
    "  Scales features to a specified range (typically [0, 1]).\n",
    "\n",
    "- **SelectKBest**:  \n",
    "  Selects the top `k` features based on a scoring function, in this case, `chi2` for feature selection.\n",
    "\n",
    "- **LogisticRegression**:  \n",
    "  A model used for classification tasks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87983752-4303-443f-9ce1-82d0b6974ddd",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd1e339-32ad-44e7-849e-6893977daf02",
   "metadata": {},
   "source": [
    "# Scikit-learn: **Pipeline vs `make_pipeline`**\n",
    "\n",
    "## 1. Definition\n",
    "\n",
    "### **Pipeline**:\n",
    "- A **Pipeline** is a Scikit-learn class used to bundle multiple steps into one single object. It sequentially applies a list of transformations and finally an estimator (e.g., a model).\n",
    "- It is more flexible than `make_pipeline`, as it allows you to explicitly define the names of each step, which can be helpful for better readability and debugging.\n",
    "\n",
    "### **`make_pipeline`**:\n",
    "- The `make_pipeline` function is a shorthand for creating a pipeline, automatically assigning names to each step based on the name of the class. It is more concise but slightly less flexible than the `Pipeline` class.\n",
    "- The names of the steps are inferred from the name of the class, which can sometimes make it harder to debug or adjust the names manually.\n",
    "\n",
    "## 2. Syntax\n",
    "\n",
    "### **Pipeline Syntax**:\n",
    "\n",
    "```python\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Define the steps of the pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='mean')),  # Step 1: Imputation\n",
    "    ('scaler', MinMaxScaler()),                  # Step 2: Scaling\n",
    "    ('model', LogisticRegression())              # Step 3: Model\n",
    "])\n",
    "\n",
    "# Fit the pipeline\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "predictions = pipeline.predict(X_test)\n",
    "```\n",
    "\n",
    "## üîß make_pipeline Syntax\n",
    "\n",
    "You explicitly define the names of each step (`'imputer'`, `'scaler'`, `'model'`).\n",
    "\n",
    "You use `fit` to train the model and `predict` to make predictions.\n",
    "\n",
    "---\n",
    "\n",
    "### Code Example:\n",
    "\n",
    "```python\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# Create the pipeline using make_pipeline\n",
    "pipeline = make_pipeline(\n",
    "    SimpleImputer(strategy='mean'),  # Step 1: Imputation\n",
    "    MinMaxScaler(),                  # Step 2: Scaling\n",
    "    LogisticRegression()             # Step 3: Model\n",
    ")\n",
    "\n",
    "# Fit the pipeline\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "predictions = pipeline.predict(X_test)\n",
    "\n",
    "```\n",
    "\n",
    "## üîÑ In this Case:\n",
    "\n",
    "- You don‚Äôt need to specify the names of each step manually. The names are automatically assigned as the lowercase of the class names (`'simpleimputer'`, `'minmaxscaler'`, `'logisticregression'`).\n",
    "- The rest of the functionality remains the same: you can fit the pipeline and predict.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b3f2a5-e77d-4aad-8aad-447597c63327",
   "metadata": {},
   "source": [
    "## ‚ö° Key Differences\n",
    "\n",
    "| **Feature**          | **Pipeline**                                  | **make_pipeline**                          |\n",
    "|----------------------|-----------------------------------------------|--------------------------------------------|\n",
    "| **Flexibility**      | More flexible as you specify step names.      | Less flexible; names are auto-assigned.    |\n",
    "| **Step Naming**      | Explicit naming of each step.                 | Step names are derived from class names.   |\n",
    "| **Readability**      | Easier to read when debugging or modifying.   | Quicker to write but harder to debug.      |\n",
    "| **Usage**            | Ideal for complex workflows.                  | Ideal for simple and quick pipelines.      |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a11fc80-73c2-4b39-bddc-e0eced87c148",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è When to Use `fit()`, `fit_transform()`, or `transform()`\n",
    "\n",
    "- **`fit()`**:  \n",
    "  You should call `fit()` when you want to fit the entire pipeline, including the model. This is typically used when the final step is model training.\n",
    "\n",
    "### Example:\n",
    "  ```python\n",
    "  pipeline.fit(X_train, y_train)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d5ee66-62bd-45dd-b8d6-81c2b5033cc7",
   "metadata": {},
   "source": [
    "- **`fit_transform()`**:  \n",
    "  If the pipeline includes preprocessing steps (like scaling or imputation) and you want to apply these transformations on the training data, you can use `fit_transform()`. This method is used when you want to both fit and transform the data.\n",
    "\n",
    "### Example:\n",
    "```python\n",
    "  X_train_transformed = pipeline.fit_transform(X_train)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37cd3c45-0b75-41bf-a8fc-f4f46b2c4c0c",
   "metadata": {},
   "source": [
    "- **`transform()`**:  \n",
    "  After the pipeline has been fitted, you can use `transform()` to transform new data (usually test data or validation data) without re-fitting the transformations (e.g., scaling). You don‚Äôt need to re-train the model when transforming data.\n",
    "\n",
    "### Example:\n",
    "```python\n",
    "  X_test_transformed = pipeline.transform(X_test)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee018230-43ae-4682-b235-eb8e1b894173",
   "metadata": {},
   "source": [
    "## üõ† Example: Combining `fit` and `fit_transform` in Pipelines\n",
    "\n",
    "### Using Pipeline with `fit` and `fit_transform`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5317aa0f-2c06-44a2-9157-c1805de09273",
   "metadata": {},
   "source": [
    "```Python\n",
    "# Using Pipeline with fit and fit_transform\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Define the pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('scaler', MinMaxScaler()),\n",
    "    ('model', LogisticRegression())\n",
    "])\n",
    "\n",
    "# Fit the pipeline on the training data\n",
    "pipeline.fit(X_train, y_train)  # Fits both the imputer and model\n",
    "\n",
    "# Transform the test set (no need to refit the model)\n",
    "X_test_transformed = pipeline.transform(X_test)\n",
    "\n",
    "# Predict using the trained model\n",
    "predictions = pipeline.predict(X_test)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8ea8196-5ccf-462f-83af-900c6dc80ba9",
   "metadata": {},
   "source": [
    "## üõ† When Not Training a Model:\n",
    "If you are not training a model, but only performing transformations (e.g., scaling, imputation), you would use `fit_transform()` to apply transformations to the data. However, the final step must be a transformation (not model fitting) in such cases.\n",
    "\n",
    "### Example of Applying Transformations Without Training a Model\n",
    "```python\n",
    "# Define the pipeline without a model\n",
    "pipeline_no_model = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('scaler', MinMaxScaler())\n",
    "])\n",
    "\n",
    "# Fit and transform training data\n",
    "X_train_transformed = pipeline_no_model.fit_transform(X_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c3ee2fa-0f20-47d5-b948-a9c2e92772a6",
   "metadata": {},
   "source": [
    "## üìã Summary\n",
    "\n",
    "- **Pipeline** is more flexible and allows you to manually specify the names of each step in the pipeline.\n",
    "- **make_pipeline** is a more concise version where the names of the steps are automatically assigned based on the class names.\n",
    "- **`fit()`** is used when fitting the entire pipeline (including the model).\n",
    "- **`fit_transform()`** is used when transformations need to be applied and the data needs to be modified.\n",
    "- If you are using a pipeline that does not include model fitting, use `fit_transform()` for transformations, but if the last step is model training, you should call `fit()`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b46f8046-4829-4904-b409-028446d435d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f292884-6ce6-49e0-a032-999f60c323a5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
