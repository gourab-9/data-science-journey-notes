{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5898346-260f-4c76-b050-fd1fbb0ea8f9",
   "metadata": {},
   "source": [
    "\n",
    "# Random Forest\n",
    "\n",
    "A **Random Forest** (or Random Decision Forest) is an ensemble learning method in machine learning that constructs multiple decision trees during training. For classification tasks, it outputs the mode (majority vote) of the individual trees; for regression, it outputs the average prediction of the trees as the final result.\n",
    "\n",
    "---\n",
    "\n",
    "## Random Forest vs. Bagging\n",
    "\n",
    "### **Bagging (Bootstrap Aggregating)**\n",
    "- Bagging is a general ensemble technique that can be applied to any machine learning model (e.g., Decision Trees, SVM, KNN).\n",
    "- Each model in bagging is trained on a different random subset of the training data, created by sampling with replacement (bootstrapping).\n",
    "- For decision trees in bagging, **all features (columns)** are considered for splitting at each node. The choice of which feature to split on is decided at the root level and applies throughout the tree.\n",
    "\n",
    "### **Random Forest**\n",
    "- Random Forest is a specific type of bagging that uses **only decision trees** as base models.\n",
    "- Like bagging, each tree is trained on a bootstrap sample (random subset of rows) from the data.\n",
    "- The key difference: **At each split (node) in a tree, Random Forest randomly selects a subset of features (columns) and determines the best split only among those features**. This random feature selection occurs at every node, not just at the root.\n",
    "- This process increases the diversity among the trees, making the ensemble more robust and less likely to overfit.\n",
    "\n",
    "---\n",
    "\n",
    "## Column (Feature) Sampling: Tree Level vs. Node Level\n",
    "\n",
    "| Method         | Column Sampling Location | Description                                                                                 |\n",
    "|----------------|-------------------------|---------------------------------------------------------------------------------------------|\n",
    "| **Bagging**    | Tree Level              | All features are available for every split in every tree.                                   |\n",
    "| **Random Forest** | Node Level           | At each node, a random subset of features is selected, and the best split is chosen among them. |\n",
    "\n",
    "- **Bagging:** For a given tree, the same set of features is used throughout the tree to decide splits.\n",
    "- **Random Forest:** For each node in a tree, a new random subset of features is chosen, and the best split is found among these. This means that different nodes in the same tree may consider different subsets of features.\n",
    "\n",
    "---\n",
    "\n",
    "## Why Random Forest Is Preferred Over Bagging with Trees\n",
    "\n",
    "- **Reduces correlation among trees:** By randomly selecting features at each split, Random Forest reduces the chance that strong predictors dominate every tree, which leads to more diverse trees and better ensemble performance[5].\n",
    "- **Improved accuracy:** The increased diversity among trees typically results in better generalization and predictive accuracy compared to standard bagging of decision trees.\n",
    "- **Handles high-dimensional data well:** Random Forest is particularly effective when there are many features, as it prevents over-reliance on any single feature.\n",
    "\n",
    "---\n",
    "\n",
    "## Summary Table\n",
    "\n",
    "| Aspect                  | Bagging                              | Random Forest                                      |\n",
    "|-------------------------|--------------------------------------|----------------------------------------------------|\n",
    "| Base Model              | Any (e.g., DT, SVM, KNN)             | Decision Trees only                                |\n",
    "| Row Sampling            | Bootstrap samples (with replacement) | Bootstrap samples (with replacement)               |\n",
    "| Feature Sampling        | All features at every split          | Random subset of features at every node            |\n",
    "| Feature Selection Level | Tree level                           | Node level                                         |\n",
    "| Model Diversity         | Lower (if strong features dominate)  | Higher (due to random feature selection per node)  |\n",
    "\n",
    "---\n",
    "\n",
    "> **In summary:**  \n",
    "> - **Bagging** aggregates predictions from multiple models trained on different data samples, using all features for splits (tree level).\n",
    "> - **Random Forest** builds on bagging by also randomly sampling features at each node (node level), resulting in more diverse and accurate decision tree ensembles.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "824501d2-8d36-4ab2-9b27-e4d133223029",
   "metadata": {},
   "source": [
    "\n",
    "# Random Forest Hyperparameters\n",
    "\n",
    "Random Forest is a powerful ensemble method that relies on building multiple decision trees and aggregating their predictions. Its performance and efficiency are heavily influenced by a set of hyperparameters, which can be tuned to balance accuracy, overfitting, and computational cost.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Forest-Level Hyperparameters\n",
    "\n",
    "These hyperparameters affect the entire forest and how trees are constructed and aggregated:\n",
    "\n",
    "- **n_estimators**  \n",
    "  The number of decision trees in the forest.  \n",
    "  - *Effect*: Increasing `n_estimators` generally improves performance and reduces overfitting, but after a certain point, gains plateau and computational cost increases.\n",
    "  - *Typical values*: 100â€“500 are common starting points.\n",
    "\n",
    "- **max_samples**  \n",
    "  The fraction (or number) of samples to draw from the dataset to train each tree (bootstrap sample).\n",
    "  - *Effect*: Controls the diversity of trees. Lower values increase diversity but may reduce stability; higher values make trees more similar.\n",
    "\n",
    "- **bootstrap**  \n",
    "  Whether sampling of data for each tree is done with replacement (`True`, default) or without replacement (`False`).\n",
    "  - *Effect*: With replacement (bootstrap=True) is standard, ensuring each tree sees a different subset of the data.\n",
    "\n",
    "- **max_features**  \n",
    "  The number of features to consider when looking for the best split at each node.\n",
    "  - *Effect*: Lower values increase tree diversity and reduce overfitting; higher values make trees more similar.  \n",
    "  - *Options*:  \n",
    "    - `\"sqrt\"` (default for classification): Square root of total features  \n",
    "    - `\"log2\"`: Log base 2 of total features  \n",
    "    - `int` or `float`: Fixed number or fraction of features.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Tree-Level Hyperparameters\n",
    "\n",
    "These hyperparameters control the structure and complexity of each individual tree:\n",
    "\n",
    "- **criterion**  \n",
    "  The function to measure the quality of a split.  \n",
    "  - *Options*: `\"gini\"`, `\"entropy\"`, `\"log_loss\"` (classification).\n",
    "\n",
    "- **max_depth**  \n",
    "  The maximum depth (number of splits from root to leaf) of each tree.\n",
    "  - *Effect*: Limits tree size to prevent overfitting. Shallow trees may underfit; very deep trees may overfit.\n",
    "\n",
    "- **min_samples_split**  \n",
    "  The minimum number of samples required to split an internal node.\n",
    "  - *Effect*: Higher values prevent trees from learning overly specific patterns (overfitting).\n",
    "\n",
    "- **min_samples_leaf**  \n",
    "  The minimum number of samples required to be at a leaf node.\n",
    "  - *Effect*: Higher values smooth the model and reduce overfitting, especially for imbalanced data.\n",
    "\n",
    "- **max_leaf_nodes**  \n",
    "  The maximum number of leaf nodes per tree.\n",
    "  - *Effect*: Limits tree complexity and helps prevent overfitting.\n",
    "\n",
    "- **min_weight_fraction_leaf**  \n",
    "  The minimum weighted fraction of the sum total of weights required to be at a leaf node.\n",
    "\n",
    "- **min_impurity_decrease**  \n",
    "  A node will be split if this split induces a decrease of the impurity greater than or equal to this value.\n",
    "\n",
    "- **ccp_alpha**  \n",
    "  Complexity parameter used for Minimal Cost-Complexity Pruning (post-pruning).\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Miscellaneous Hyperparameters\n",
    "\n",
    "- **n_jobs**  \n",
    "  Number of parallel jobs for training.  \n",
    "  - *Effect*: `-1` uses all available cores, speeding up training on multicore systems.\n",
    "\n",
    "- **random_state**  \n",
    "  Controls randomness for reproducibility.  \n",
    "  - *Effect*: Ensures results can be replicated with the same data and parameters.\n",
    "\n",
    "- **verbose**  \n",
    "  Controls the level of messages printed during training.\n",
    "\n",
    "- **warm_start**  \n",
    "  If `True`, reuse the solution of the previous call to fit and add more estimators to the ensemble, useful for incremental training.\n",
    "\n",
    "- **class_weight**  \n",
    "  Adjusts weights for classes to handle imbalanced datasets.\n",
    "\n",
    "- **oob_score**  \n",
    "  Whether to use out-of-bag samples to estimate the generalization accuracy.\n",
    "  - *Effect*: Provides a built-in cross-validation estimate without needing a separate validation set.\n",
    "\n",
    "---\n",
    "\n",
    "## Out-of-Bag (OOB) Samples\n",
    "\n",
    "- **OOB samples** are data points not selected in the bootstrap sample for a given tree.\n",
    "- These samples are used to estimate the model's performance, similar to cross-validation, without additional computational cost.\n",
    "\n",
    "---\n",
    "\n",
    "## Advantages of Random Forest\n",
    "\n",
    "- **Robustness to overfitting** (especially compared to single trees)\n",
    "- **Handles large datasets and high-dimensional data**\n",
    "- **Estimates feature importance**\n",
    "- **Parallelizable** (can utilize multiple CPU cores)\n",
    "- **Non-parametric** (no strong assumptions about data distribution)\n",
    "- **Works well with both classification and regression tasks**\n",
    "\n",
    "---\n",
    "\n",
    "## Disadvantages of Random Forest\n",
    "\n",
    "- **Less interpretable** than single decision trees (black box nature).\n",
    "- **Can be inefficient** with very sparse or high-cardinality data.\n",
    "- **May require significant memory** for large forests.\n",
    "- **Performance may degrade** on highly imbalanced datasets without proper class weighting.\n",
    "- **Can overfit** if the number of trees is too high relative to data size or if trees are too deep.\n",
    "- **Feature importance** can be biased if features are highly correlated.\n",
    "\n",
    "---\n",
    "\n",
    "## Assumptions & Limitations\n",
    "\n",
    "- **Independence of Trees**: Assumes individual trees are independent; correlated features can reduce diversity.\n",
    "- **Sufficient Data**: Requires enough data to create diverse bootstrap samples.\n",
    "- **Bagging Effectiveness**: Relies on the principle that aggregating weak learners improves overall performance.\n",
    "\n",
    "---\n",
    "\n",
    "## Hyperparameter Tuning Tips\n",
    "\n",
    "- Use tools like **GridSearchCV** or **RandomizedSearchCV** for systematic tuning.\n",
    "- Start with default values, then incrementally adjust key hyperparameters (`n_estimators`, `max_depth`, `max_features`, `min_samples_leaf`, etc.).\n",
    "- Monitor both accuracy and computation time to find the best trade-off for your application.\n",
    "\n",
    "---\n",
    "\n",
    "> **Summary:**  \n",
    "> Random Forest hyperparameters are crucial for balancing model accuracy, generalization, and computational efficiency. Proper tuning-especially of `n_estimators`, `max_depth`, `max_features`, and `min_samples_leaf`-can significantly enhance performance, while parameters like `n_jobs` and `oob_score` improve training efficiency and evaluation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "145c85bc-0084-4e7c-9a10-86f948201456",
   "metadata": {},
   "source": [
    "```python\n",
    "# 1. Basic Random Forest Classifier Example\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    \n",
    "    # X, y = your features and target arrays\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=44)\n",
    "    \n",
    "    rf_model = RandomForestClassifier(n_estimators=50, random_state=44)\n",
    "    rf_model.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred = rf_model.predict(X_test)\n",
    "    \n",
    "    # 2. Basic Random Forest Regressor Example\n",
    "    from sklearn.ensemble import RandomForestRegressor\n",
    "    \n",
    "    regressor = RandomForestRegressor(n_estimators=100, random_state=44)\n",
    "    regressor.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred = regressor.predict(X_test)\n",
    "    \n",
    "    # 3. Hyperparameter Tuning with RandomizedSearchCV (Classifier)\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn.model_selection import RandomizedSearchCV\n",
    "    from scipy.stats import randint\n",
    "    \n",
    "    param_dist = {\n",
    "        'n_estimators': randint(50, 500),\n",
    "        'max_depth': randint(1, 20)\n",
    "    }\n",
    "    \n",
    "    rf = RandomForestClassifier()\n",
    "    rand_search = RandomizedSearchCV(\n",
    "        rf,\n",
    "        param_distributions=param_dist,\n",
    "        n_iter=5,\n",
    "        cv=5\n",
    "    )\n",
    "    rand_search.fit(X_train, y_train)\n",
    "    \n",
    "    best_rf = rand_search.best_estimator_\n",
    "    print('Best hyperparameters:', rand_search.best_params_)\n",
    "    \n",
    "    \n",
    "    # 4. Hyperparameter Tuning with RandomizedSearchCV (Regressor)\n",
    "    from sklearn.ensemble import RandomForestRegressor\n",
    "    from sklearn.model_selection import RandomizedSearchCV\n",
    "    from scipy.stats import randint\n",
    "    \n",
    "    random_grid = {\n",
    "        'bootstrap': [True, False],\n",
    "        'max_depth': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, None],\n",
    "        'max_features': ['auto', 'sqrt'],\n",
    "        'min_samples_leaf': [1, 2, 4],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'n_estimators': [130, 180, 230]\n",
    "    }\n",
    "    \n",
    "    rf = RandomForestRegressor()\n",
    "    rf_random = RandomizedSearchCV(\n",
    "        estimator=rf,\n",
    "        param_distributions=random_grid,\n",
    "        n_iter=100,\n",
    "        cv=3,\n",
    "        verbose=2,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    rf_random.fit(X_train, y_train)\n",
    "    \n",
    "    print(rf_random.best_params_)\n",
    "    print(rf_random.best_score_)\n",
    "    print(rf_random.best_estimator_)\n",
    "    \n",
    "    \n",
    "    # 5. Example with Synthetic Data (Classifier)\n",
    "    from sklearn.datasets import make_classification\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    \n",
    "    X, y = make_classification(n_samples=1000, n_features=4, n_informative=2, n_redundant=0, random_state=0, shuffle=False)\n",
    "    clf = RandomForestClassifier(max_depth=2, random_state=0)\n",
    "    clf.fit(X, y)\n",
    "    print(clf.predict([[0, 0, 0, 0]]))\n",
    "    \n",
    "    \n",
    "    # 6. Common Parameters for RandomForestClassifier (for reference)\n",
    "    rf = RandomForestClassifier(\n",
    "        n_estimators=100,\n",
    "        criterion='gini',\n",
    "        max_depth=None,\n",
    "        min_samples_split=2,\n",
    "        min_samples_leaf=1,\n",
    "        max_features='sqrt',\n",
    "        bootstrap=True,\n",
    "        oob_score=False,\n",
    "        n_jobs=-1,\n",
    "        random_state=42,\n",
    "        verbose=1\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ccb0da-4215-4478-b44b-924aa7113f06",
   "metadata": {},
   "source": [
    "# How Random Forest Works in Classification and Regression\n",
    "\n",
    "Random Forest is a versatile ensemble learning algorithm that can handle both **classification** and **regression** tasks by combining the outputs of multiple decision trees built on random subsets of data and features.\n",
    "\n",
    "---\n",
    "\n",
    "## Core Workflow (Both Tasks)\n",
    "\n",
    "1. **Bootstrap Sampling**: For each tree, a random sample of the data (with replacement) is drawn-this is called bootstrapping.\n",
    "2. **Random Feature Selection**: At each node split in a tree, a random subset of features is considered, not all features.\n",
    "3. **Tree Building**: Each decision tree is grown independently using its bootstrap sample and random feature selection at each split.\n",
    "4. **Prediction Aggregation**: The predictions from all trees are combined to produce the final output, but the aggregation method differs for classification and regression.\n",
    "\n",
    "---\n",
    "\n",
    "## Random Forest for Classification\n",
    "\n",
    "- **Purpose:** Assigns input data to a discrete class label.\n",
    "- **Process:**\n",
    "  - Each tree in the forest predicts a class label for the input.\n",
    "  - The final prediction is the **mode** (majority vote) of all tree predictions.\n",
    "- **Example:** Predicting if an email is spam or not, or classifying an image as a cat or dog.\n",
    "\n",
    "**Visualization:**\n",
    "- Tree 1: predicts \"A\"\n",
    "- Tree 2: predicts \"B\"\n",
    "- Tree 3: predicts \"A\"\n",
    "- Final prediction = Mode([\"A\", \"B\", \"A\"]) = \"A\"\n",
    "\n",
    "- **Benefits:** Reduces overfitting, improves accuracy, and works well with both categorical and numerical features.\n",
    "\n",
    "---\n",
    "\n",
    "## Random Forest for Regression\n",
    "\n",
    "- **Purpose:** Predicts a continuous numerical value.\n",
    "- **Process:**\n",
    "  - Each tree in the forest outputs a numerical prediction for the input.\n",
    "  - The final prediction is the **mean** (average) of all tree predictions.\n",
    "- **Example:** Predicting house prices, stock values, or temperature.\n",
    "\n",
    "**Visualization:**\n",
    "- Tree 1: predicts 100\n",
    "- Tree 2: predicts 120\n",
    "- Tree 3: predicts 110\n",
    "- Final prediction = Mean([100, 110,120])\n",
    "\n",
    "\n",
    "- **Benefits:** Handles non-linear relationships, robust to outliers, and reduces variance compared to a single tree.\n",
    "\n",
    "---\n",
    "\n",
    "## Key Differences in Aggregation\n",
    "\n",
    "| Task            | Tree Output     | Aggregation Method        | Final Output                |\n",
    "|-----------------|----------------|--------------------------|-----------------------------|\n",
    "| Classification  | Class label    | Majority vote (mode)     | Most common class           |\n",
    "| Regression      | Numeric value  | Mean (average)           | Average of all predictions  |\n",
    "\n",
    "---\n",
    "\n",
    "## Additional Notes\n",
    "\n",
    "- **Randomness** (in both row and feature selection) ensures trees are de-correlated and the ensemble is robust.\n",
    "- **Hyperparameters** like number of trees (`n_estimators`), max features, and tree depth can be tuned for optimal results.\n",
    "- **Feature Importance:** Random Forest can estimate which features are most important for prediction.\n",
    "\n",
    "---\n",
    "\n",
    "> **Summary:**  \n",
    "> - For **classification**, Random Forest predicts the most frequent class among all trees (majority vote).\n",
    "> - For **regression**, it predicts the average value from all trees.\n",
    "> - The ensemble approach reduces overfitting, increases accuracy, and is effective for both categorical and continuous prediction tasks.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
