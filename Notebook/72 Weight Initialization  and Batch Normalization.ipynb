{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9724e80e-9528-46c3-83a8-936a3895a98c",
   "metadata": {},
   "source": [
    "# 🧠 Weight Initialization in Neural Networks\n",
    "\n",
    "Weight initialization is a critical step in training deep neural networks. The right initialization helps:\n",
    "- Prevent **vanishing/exploding gradients**\n",
    "- Ensure **symmetry breaking**\n",
    "- Enable **faster convergence**\n",
    "\n",
    "---\n",
    "\n",
    "## 🚦 Working of Neural Networks – Step-by-Step\n",
    "\n",
    "1. **Initialize Parameters**: Assign initial values to all weights \\( W \\) and biases \\( b \\).\n",
    "2. **Choose an Optimization Algorithm**: e.g., SGD, Adam, RMSprop, etc.\n",
    "3. **Repeat Until Convergence**:\n",
    "   - (a) **Forward Propagation**: Compute layer-wise activations.\n",
    "   - (b) **Compute Cost Function**: Evaluate prediction loss.\n",
    "   - (c) **Backpropagation**: Compute gradients of cost w.r.t. weights.\n",
    "   - (d) **Update Parameters**: Use gradients to update weights using the chosen optimizer.\n",
    "\n",
    "---\n",
    "\n",
    "## ❗ Why Initialization Matters\n",
    "\n",
    "Improper weight initialization can lead to the following issues:\n",
    "\n",
    "- **Vanishing Gradients**: Gradients become too small to make useful updates.\n",
    "- **Exploding Gradients**: Gradients become too large, leading to instability.\n",
    "- **Symmetry Breaking Failure**: If all neurons start with the same values, they learn the same features.\n",
    "- **Slow Convergence**: Small or large weights make learning inefficient.\n",
    "\n",
    "---\n",
    "\n",
    "## ⚠️ What NOT To Do\n",
    "\n",
    "| Method                       | Problem                                                                                       |\n",
    "|-----------------------------|-----------------------------------------------------------------------------------------------|\n",
    "| **Zero Initialization**     | All neurons start identically → no symmetry breaking. No learning.                           |\n",
    "| **Constant Initialization** | Still symmetric → same update for all neurons.                                               |\n",
    "| **Very Small Weights**      | Can lead to **vanishing gradients** (especially with sigmoid/tanh).                          |\n",
    "| **Very Large Weights**      | Can lead to **exploding gradients** (especially with ReLU) → unstable training.              |\n",
    "\n",
    "---\n",
    "\n",
    "## ✅ Practical Initialization Techniques\n",
    "\n",
    "### 1. Xavier / Glorot Initialization\n",
    "\n",
    "- **Best for**: Sigmoid / Tanh\n",
    "- **Goal**: Maintain equal variance of activations and gradients across layers.\n",
    "- **Formulas**:\n",
    "\n",
    "**Normal Distribution:**\n",
    "$$\n",
    "W \\sim \\mathcal{N}\\left(0, \\frac{1}{n_{\\text{in}}}\\right)\n",
    "$$\n",
    "\n",
    "**Uniform Distribution:**\n",
    "$$\n",
    "W \\sim \\mathcal{U}\\left[-\\sqrt{\\frac{6}{n_{\\text{in}} + n_{\\text{out}}}}, \\sqrt{\\frac{6}{n_{\\text{in}} + n_{\\text{out}}}}\\right]\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### 2. He Initialization\n",
    "\n",
    "- **Best for**: ReLU / Leaky ReLU\n",
    "- **Goal**: Maintain large enough activations to avoid vanishing gradients.\n",
    "- **Formulas**:\n",
    "\n",
    "**Normal Distribution:**\n",
    "$$\n",
    "W \\sim \\mathcal{N}\\left(0, \\frac{2}{n_{\\text{in}}}\\right)\n",
    "$$\n",
    "\n",
    "**Uniform Distribution:**\n",
    "$$\n",
    "W \\sim \\mathcal{U}\\left[-\\sqrt{\\frac{6}{n_{\\text{in}}}}, \\sqrt{\\frac{6}{n_{\\text{in}}}}\\right]\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "### 🔬 Summary Table\n",
    "\n",
    "| Initialization        | Distribution Type | Formula                                                                                                             | Use Case           |\n",
    "|------------------------|-------------------|----------------------------------------------------------------------------------------------------------------------|---------------------|\n",
    "| **Xavier (Glorot)**    | Normal             | $$W \\sim \\mathcal{N}\\left(0, \\frac{1}{n_{\\text{in}}}\\right)$$                                                       | Sigmoid / Tanh      |\n",
    "|                        | Uniform            | $$W \\sim \\mathcal{U}\\left[-\\sqrt{\\frac{6}{n_{\\text{in}} + n_{\\text{out}}}}, \\sqrt{\\frac{6}{n_{\\text{in}} + n_{\\text{out}}}}\\right]$$ |                     |\n",
    "| **He**                 | Normal             | $$W \\sim \\mathcal{N}\\left(0, \\frac{2}{n_{\\text{in}}}\\right)$$                                                       | ReLU / Leaky ReLU   |\n",
    "|                        | Uniform            | $$W \\sim \\mathcal{U}\\left[-\\sqrt{\\frac{6}{n_{\\text{in}}}}, \\sqrt{\\frac{6}{n_{\\text{in}}}}\\right]$$                  |                     |\n",
    "| **Zero Init**          | -                  | $$W = 0$$                                                                                                            | ❌ Never use         |\n",
    "| **Constant Init**      | -                  | $$W = c \\neq 0$$                                                                                                     | ❌ Still symmetric    |\n",
    "\n",
    "---\n",
    "\n",
    "## 🧪 Python Code: Weight Initialization Examples\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "# Xavier Initialization (Uniform)\n",
    "def xavier_init(shape):\n",
    "    limit = np.sqrt(6 / sum(shape))\n",
    "    return np.random.uniform(-limit, limit, size=shape)\n",
    "\n",
    "# He Initialization (Normal)\n",
    "def he_init(shape):\n",
    "    std_dev = np.sqrt(2 / shape[0])\n",
    "    return np.random.normal(0, std_dev, size=shape)\n",
    "\n",
    "# Example usage:\n",
    "W1 = xavier_init((256, 128))\n",
    "W2 = he_init((256, 128))\n",
    "\n",
    "print(\"Xavier Init - Mean:\", W1.mean(), \"Std:\", W1.std())\n",
    "print(\"He Init     - Mean:\", W2.mean(), \"Std:\", W2.std())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a1eb8b-d35a-4230-aef5-5380f6ebc3d2",
   "metadata": {},
   "source": [
    "# 📦 Batch Normalization\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "## 🧠 What is Batch Normalization?\r\n",
    "\r\n",
    "Batch Normalization (BN) is a **technique to normalize the activations of each layer** in a neural network across a mini-batch. It helps:\r\n",
    "- Accelerate training.\r\n",
    "- Make optimization more stable.\r\n",
    "- Reduce sensitivity to initialization and learning rates.\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "## ⚙️ Key Idea\r\n",
    "\r\n",
    "In neural networks, **each layer’s output (activations)** is passed to the next layer as input. Batch normalization normalizes these activations for each mini-batch to have:\r\n",
    "- **Mean ≈ 0**\r\n",
    "- **Standard Deviation ≈ 1**\r\n",
    "\r\n",
    "> This ensures a stable distribution of data across layers, preventing issues like internal covariate shift.\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "## 🎯 Why Use Batch Normalization?\r\n",
    "\r\n",
    "### 📉 Internal Covariate Shift\r\n",
    "- Refers to changes in the distribution of hidden layer activations during training as weights update.\r\n",
    "- BN reduces this shift, keeping the distribution more stable.\r\n",
    "\r\n",
    "### 💡 Benefits of Batch Normalization\r\n",
    "\r\n",
    "| Advantage                      | Explanation                                                                 |\r\n",
    "|-------------------------------|-----------------------------------------------------------------------------|\r\n",
    "| ✅ Faster Training             | Allows the use of **higher learning rates** without risk of divergence.     |\r\n",
    "| ✅ More Stable                 | Reduces sensitivity to weight initialization.                              |\r\n",
    "| ✅ Regularization              | Acts as a **mild regularizer**, helping reduce overfitting.                |\r\n",
    "| ✅ Less Dependence on Init     | Reduces reliance on perfect weight initialization methods.                 |\r\n",
    "| ✅ Reduces Vanishing Gradients | Especially in deep networks.                                               |\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "## 🔬 How It Works – Step-by-Step\r\n",
    "\r\n",
    "For each neuron activation `x` in a mini-batch:\r\n",
    "\r\n",
    "1. **Compute Mean and Variance**:\r\n",
    "\r\n",
    "   $$\r\n",
    "   \\mu = \\frac{1}{m} \\sum_{i=1}^{m} x_i \\quad ; \\quad \\sigma^2 = \\frac{1}{m} \\sum_{i=1}^{m} (x_i - \\mu)^2\r\n",
    "   $$\r\n",
    "\r\n",
    "2. **Normalize**:\r\n",
    "\r\n",
    "   $$\r\n",
    "   \\hat{x}_i = \\frac{x_i - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}}\r\n",
    "   $$\r\n",
    "\r\n",
    "3. **Scale and Shift** (using learnable parameters $\\gamma$, $\\beta$):\r\n",
    "\r\n",
    "   $$\r\n",
    "   y_i = \\gamma \\hat{x}_i + \\beta\r\n",
    "   $$\r\n",
    "\r\n",
    "- $\\epsilon$: small constant for numerical stability  \r\n",
    "- $\\gamma$, $\\beta$: **learned** during training\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "## 📦 Parameters in Batch Normalization\r\n",
    "\r\n",
    "For a layer with **n neurons**, Batch Normalization introduces **4 parameters per neuron**:\r\n",
    "\r\n",
    "### ✅ Learnable Parameters:\r\n",
    "- $\\gamma$ (scale)\r\n",
    "- $\\beta$ (shift)\r\n",
    "\r\n",
    "These are **trainable** and updated during backpropagation.\r\n",
    "\r\n",
    "### 📈 Non-learnable Parameters:\r\n",
    "- Running mean: $\\mu$\r\n",
    "- Running variance: $\\sigma^2$\r\n",
    "\r\n",
    "These are **computed during training** and **used during inference**.\r\n",
    "\r\n",
    "**Example**: For 3 neurons  \r\n",
    "- Learnable: $3 \\times 2 = 6$  \r\n",
    "- Non-learnable: $3 \\times 2 = 6$\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "## 🛠️ During Training vs Inference\r\n",
    "\r\n",
    "- **Training**: Use batch-wise mean and variance  \r\n",
    "- **Inference**: Use **running (moving average)** of mean and variance from training\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "## 🧪 Keras Implementation\r\n",
    "\r\n",
    "```python\r\n",
    "from keras.models import Sequential\r\n",
    "from keras.layers import Dense, BatchNormalization\r\n",
    "\r\n",
    "model = Sequential()\r\n",
    "model.add(Dense(3, activation='relu', input_dim=2))\r\n",
    "model.add(BatchNormalization())   # After activation\r\n",
    "model.add(Dense(2, activation='relu'))\r\n",
    "model.add(BatchNormalization())\r\n",
    "model.add(Dense(1, activation='sigmoid'))\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0927e35-d916-43f6-812d-01df61f87c25",
   "metadata": {},
   "source": [
    "---\r\n",
    "\r\n",
    "## 📊 Parameters in Batch Normalization\r\n",
    "\r\n",
    "For a layer with **n neurons**, Batch Normalization introduces **4 parameters per neuron**:\r\n",
    "\r\n",
    "### ✅ Learnable Parameters:\r\n",
    "- $\\gamma$ (scale)\r\n",
    "- $\\beta$ (shift)\r\n",
    "\r\n",
    "These are **trainable** and updated during backpropagation.\r\n",
    "\r\n",
    "### 📈 Non-learnable Parameters:\r\n",
    "- Running **mean** $\\mu$\r\n",
    "- Running **variance** $\\sigma^2$\r\n",
    "\r\n",
    "These are **calculated and stored** during training, and **used during inference** to normalize activations.\r\n",
    "\r\n",
    "### 🔢 Example:\r\n",
    "\r\n",
    "If a layer has **3 neurons**, then:\r\n",
    "\r\n",
    "- **Learnable** parameters = $3 \\times 2 = 6$  \r\n",
    "- **Non-learnable** parameters = $3 \\times 2 = 6$\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "## ✅ Summary\r\n",
    "\r\n",
    "| Feature                         | Description                                           |\r\n",
    "| ------------------------------- | ----------------------------------------------------- |\r\n",
    "| 🧠 **Faster Training**          | Allows use of higher learning rates                  |\r\n",
    "| 🔁 **Stable Gradients**         | Helps prevent vanishing or exploding gradients       |\r\n",
    "| ⚙️ **Reduced Init Sensitivity** | Reduces reliance on careful weight initialization    |\r\n",
    "| 🔐 **Acts as Regularizer**      | Provides regularization effect (may reduce dropout)  |\r\n",
    "| 🌐 **Improved Generalization**  | Often improves test performance and reduces overfitting |\r\n",
    "\r\n",
    "> 🧠 **Tip**: BatchNorm is typically applied **after the activation**  \r\n",
    "> (`Dense → Activation → BatchNorm`) but can also be applied **before activation** in some architectures.\r\n",
    "\r\n",
    "---\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c6d242-34d4-4aaa-882f-48ff5a29aab7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
